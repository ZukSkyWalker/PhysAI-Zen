{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81459f58-ef66-4dac-ba7a-90b6e3a0c20f",
   "metadata": {},
   "source": [
    "# Chapter 02: Decision Transformer ‚Äì Offline RL as Discrete Path Integral\n",
    "\n",
    "> \"The agent does not learn a policy or a value function.  \n",
    "> It learns the partition function over all possible futures, weighted by exponential reward.\"\n",
    "\n",
    "‰∏Ä‰∏™GPT-style TransformerÂÖ∂ÂÆûÂ∞±ÊòØÁ¶ªÊï£Ë∑ØÂæÑÁßØÂàÜÔºö\n",
    "\n",
    "$$P(trajectory) \\propto exp(Return(\\tau))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6079af64-521b-46ef-8e6b-a5a139924ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, random_split\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79098a69-3693-4f72-a4a1-477874ff34c4",
   "metadata": {},
   "source": [
    "### 1. Data Preparation: Building the Discrete Path-Integral Ensemble\n",
    "\n",
    "In the path-integral picture, the \"dataset\" is nothing but a Monte Carlo estimate of the trajectory partition function\n",
    "$$Z = \\sum_{\\tau} \\exp\\left( \\sum_t \\gamma^t r_t \\right) \\approx \\sum_{i=1}^N \\exp\\big(G(\\tau_i)\\big)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe63d7c6-0b22-47da-9c2c-44c6c5712cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_to_go(rewards: torch.Tensor, gamma: float = 0.99) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute return-to-go (RTG) for each timestep in a trajectory.\n",
    "    \n",
    "    RTG represents the expected cumulative future reward from each timestep.\n",
    "    This is a KEY conditioning signal for the Decision Transformer.\n",
    "    \n",
    "    Args:\n",
    "        rewards: (T,) tensor of per-timestep rewards\n",
    "        gamma: discount factor for future rewards\n",
    "    \n",
    "    Returns:\n",
    "        rtg: (T,) tensor where rtg[t] = sum_{k=t}^T gamma^{k-t} * r_k\n",
    "    \n",
    "    Example:\n",
    "        If rewards = [1, 1, 1] and gamma = 0.99:\n",
    "        rtg = [1 + 0.99 + 0.99^2, 1 + 0.99, 1] = [2.9701, 1.99, 1.0]\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    rtg = torch.zeros_like(rewards)\n",
    "    \n",
    "    # Compute RTG by backward accumulation\n",
    "    # Start from the end: rtg[T-1] = rewards[T-1]\n",
    "    # Work backwards: rtg[t] = rewards[t] + gamma * rtg[t+1]\n",
    "    cumulative = 0.0\n",
    "    for t in reversed(range(T)):\n",
    "        cumulative = rewards[t] + gamma * cumulative\n",
    "        rtg[t] = cumulative\n",
    "    \n",
    "    return rtg  # (T,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "026de9d7-6b85-4976-982f-0f0addd65396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectories(\n",
    "    env_name: str = \"CartPole-v1\",\n",
    "    n_trajectories: int = 1000,\n",
    "    max_length: int = 1000,\n",
    "    target_rtg_range: tuple = (0, 500),  # Target RTG range for this collection\n",
    "    gamma: float = 0.99,\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect trajectories with RTG-conditional behavior.\n",
    "    \n",
    "    KEY IDEA: To teach the model RTG conditioning, we generate data where\n",
    "    the same state leads to different actions depending on the target RTG.\n",
    "    \n",
    "    Strategy:\n",
    "    - Low RTG target (0-100): Use poor policy (high randomness)\n",
    "    - Medium RTG target (100-300): Use medium policy (moderate randomness)\n",
    "    - High RTG target (300-500): Use expert policy (low randomness)\n",
    "    \n",
    "    Args:\n",
    "        env_name: Gymnasium environment name\n",
    "        n_trajectories: Number of episodes to collect\n",
    "        max_length: Maximum steps per episode\n",
    "        target_rtg_range: (min_rtg, max_rtg) - range of target RTGs to sample from\n",
    "        gamma: Discount factor\n",
    "    \n",
    "    Returns:\n",
    "        trajectories: List of dicts with 'states', 'actions', 'rewards', 'returns', 'target_rtg'\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    trajectories = []\n",
    "    \n",
    "    min_rtg, max_rtg = target_rtg_range\n",
    "    \n",
    "    for _ in tqdm(range(n_trajectories), desc=f\"Collecting RTG-conditional data\"):\n",
    "        # Sample a policy quality level\n",
    "        # We'll assign the target RTG AFTER seeing the actual return\n",
    "        policy_quality = np.random.choice(['poor', 'medium', 'expert'], p=[0.3, 0.4, 0.3])\n",
    "        \n",
    "        # Determine randomness rate based on policy quality\n",
    "        if policy_quality == 'poor':\n",
    "            random_rate = 0.6  # 60% random ‚Üí expect low returns\n",
    "        elif policy_quality == 'medium':\n",
    "            random_rate = 0.3  # 30% random ‚Üí expect medium returns\n",
    "        else:  # expert\n",
    "            random_rate = 0.1  # 10% random ‚Üí expect high returns\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        states, actions, rewards = [], [], []\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            # Apply RTG-conditional policy\n",
    "            if np.random.random() < random_rate:\n",
    "                # Random action (exploration based on target RTG)\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Use expert heuristic\n",
    "                if \"CartPole\" in env_name:\n",
    "                    angle = obs[2]\n",
    "                    angular_velocity = obs[3]\n",
    "                    # Expert heuristic: balance the pole\n",
    "                    action = 1 if (angle + 0.3 * angular_velocity) > 0 else 0\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "            \n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            states.append(torch.tensor(obs, dtype=torch.float32))\n",
    "            \n",
    "            # For discrete actions, store as integer\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                actions.append(torch.tensor(action, dtype=torch.long))\n",
    "            else:\n",
    "                actions.append(torch.tensor(action, dtype=torch.float32))\n",
    "            \n",
    "            rewards.append(torch.tensor(reward, dtype=torch.float32))\n",
    "            \n",
    "            obs = next_obs\n",
    "            if done or truncated:\n",
    "                break\n",
    "        \n",
    "        states = torch.stack(states)\n",
    "        if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            # For discrete actions, keep as long tensor\n",
    "            actions = torch.stack(actions).unsqueeze(-1)  # (T, 1)\n",
    "        else:\n",
    "            # For continuous actions, stack directly\n",
    "            actions = torch.stack(actions)\n",
    "        rewards = torch.stack(rewards)\n",
    "        \n",
    "        # CRITICAL FIX: Assign target_rtg based on ACTUAL achieved return\n",
    "        # This ensures RTG correctly correlates with trajectory quality!\n",
    "        actual_return = rewards.sum().item()\n",
    "        \n",
    "        trajectories.append({\n",
    "            \"states\": states,\n",
    "            \"actions\": actions,\n",
    "            \"rewards\": rewards,\n",
    "            \"returns\": actual_return,\n",
    "            \"target_rtg\": actual_return,  # Use actual return as target RTG\n",
    "            \"policy_quality\": policy_quality,  # Store for analysis\n",
    "        })\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Print statistics\n",
    "    returns = [t['returns'] for t in trajectories]\n",
    "    target_rtgs = [t['target_rtg'] for t in trajectories]\n",
    "    poor_trajs = [t for t in trajectories if t['policy_quality'] == 'poor']\n",
    "    medium_trajs = [t for t in trajectories if t['policy_quality'] == 'medium']\n",
    "    expert_trajs = [t for t in trajectories if t['policy_quality'] == 'expert']\n",
    "    \n",
    "    print(f\"\\nCollected {len(trajectories)} trajectories:\")\n",
    "    print(f\"  Policy distribution: {len(poor_trajs)} poor, {len(medium_trajs)} medium, {len(expert_trajs)} expert\")\n",
    "    print(f\"  Poor policy:   mean return = {np.mean([t['returns'] for t in poor_trajs]):.1f}\")\n",
    "    print(f\"  Medium policy: mean return = {np.mean([t['returns'] for t in medium_trajs]):.1f}\")\n",
    "    print(f\"  Expert policy: mean return = {np.mean([t['returns'] for t in expert_trajs]):.1f}\")\n",
    "    print(f\"  Overall: {np.mean(returns):.1f} ¬± {np.std(returns):.1f}, range=[{np.min(returns):.0f}, {np.max(returns):.0f}]\")\n",
    "    print(f\"  ‚úÖ Target RTG = Actual return (perfect correlation by design)\")\n",
    "    \n",
    "    return trajectories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7063bb4d-e448-4e8f-ba7e-c5f0d91d25e8",
   "metadata": {},
   "source": [
    "So here we collect RTG-conditional data across the full spectrum, and we are going to build a model learning from mixed-quality offline data to reproduce expert behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26c36e7-c7c7-4e28-a158-1a37610a2919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ddd9d7902146788b5723c40cfb64d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Collecting RTG-conditional data:   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 2000 trajectories:\n",
      "  Policy distribution: 664 poor, 753 medium, 583 expert\n",
      "  Poor policy:   mean return = 94.8\n",
      "  Medium policy: mean return = 377.4\n",
      "  Expert policy: mean return = 489.3\n",
      "  Overall: 316.2 ¬± 184.5, range=[9, 500]\n",
      "  ‚úÖ Target RTG = Actual return (perfect correlation by design)\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Dataset Statistics\n",
      "======================================================================\n",
      "Total trajectories: 2000\n",
      "Return range: [9.0, 500.0]\n",
      "Return mean: 316.2 ¬± 184.5\n",
      "Return percentiles:\n",
      "  25th: 133.8\n",
      "  50th: 354.0\n",
      "  75th: 500.0\n",
      "  90th: 500.0\n"
     ]
    }
   ],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "\n",
    "trajectories = collect_trajectories(\n",
    "    env_name=env_name,\n",
    "    n_trajectories=2000,  # Collect diverse data across all RTG levels\n",
    "    target_rtg_range=(0, 500),\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "# Print dataset statistics\n",
    "returns = [t['returns'] for t in trajectories]\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STEP 2: Dataset Statistics\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total trajectories: {len(trajectories)}\")\n",
    "print(f\"Return range: [{min(returns):.1f}, {max(returns):.1f}]\")\n",
    "print(f\"Return mean: {np.mean(returns):.1f} ¬± {np.std(returns):.1f}\")\n",
    "print(f\"Return percentiles:\")\n",
    "print(f\"  25th: {np.percentile(returns, 25):.1f}\")\n",
    "print(f\"  50th: {np.percentile(returns, 50):.1f}\")\n",
    "print(f\"  75th: {np.percentile(returns, 75):.1f}\")\n",
    "print(f\"  90th: {np.percentile(returns, 90):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49fdbae-5c6c-4c9d-9166-af3f148a3a54",
   "metadata": {},
   "source": [
    "Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc200765-2f0b-4fb3-ae07-9d91af66245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Decision Transformer training.\n",
    "    \n",
    "    Converts trajectories into fixed-length context windows of (state, action, RTG) tuples.\n",
    "    Each sample is a sliding window of `context_length` consecutive timesteps.\n",
    "    \n",
    "    This design allows the transformer to attend over recent history when predicting actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectories, context_length: int = 20, gamma: float = 0.99):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trajectories: List of dicts with keys 'states', 'actions', 'rewards'\n",
    "            context_length: Number of timesteps in each training sample\n",
    "            gamma: Discount factor for computing RTG\n",
    "        \"\"\"\n",
    "        self.context_length = context_length\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # We'll store all training samples as fixed-length sequences\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rtg = []\n",
    "        self.timesteps = []\n",
    "        \n",
    "        # Process each trajectory into multiple training samples\n",
    "        for traj in trajectories:\n",
    "            states = traj['states']      # (T, state_dim)\n",
    "            actions = traj['actions']    # (T, action_dim)\n",
    "            rewards = traj['rewards']    # (T,) - 1D tensor\n",
    "            \n",
    "            T = len(states)\n",
    "            # Skip trajectories that are too short\n",
    "            if T <= context_length:\n",
    "                continue\n",
    "            \n",
    "            # Compute return-to-go for this trajectory\n",
    "            rtg = compute_returns_to_go(rewards, gamma)  # (T,)\n",
    "            \n",
    "            # Create sliding windows of context_length\n",
    "            # For a trajectory of length T, we get (T - context_length + 1) samples\n",
    "            for start in range(T - context_length + 1):\n",
    "                end = start + context_length\n",
    "                \n",
    "                self.states.append(states[start:end])\n",
    "                self.actions.append(actions[start:end])\n",
    "                self.rtg.append(rtg[start:end])  # (context_length,)\n",
    "                self.timesteps.append(torch.arange(start, end))\n",
    "        \n",
    "        # Stack into tensors for efficient batching\n",
    "        self.states = torch.stack(self.states)              # (N, L, state_dim)\n",
    "        self.actions = torch.stack(self.actions)            # (N, L, action_dim)\n",
    "        self.rtg = torch.stack(self.rtg).unsqueeze(-1)      # (N, L, 1)\n",
    "        self.timesteps = torch.stack(self.timesteps)        # (N, L)\n",
    "        \n",
    "        print(f\"Built dataset with {len(self)} sequences (context_length={context_length})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.states[idx],\n",
    "            self.actions[idx],\n",
    "            self.rtg[idx],\n",
    "            self.timesteps[idx],\n",
    "            torch.ones(self.context_length, dtype=torch.bool)\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def state_dim(self):\n",
    "        return self.states.shape[-1]\n",
    "    \n",
    "    @property\n",
    "    def action_dim(self):\n",
    "        return self.actions.shape[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "826d97fd-70f3-4e2a-b4ee-67fc7149bf6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built dataset with 594689 sequences (context_length=20)\n",
      "\n",
      "RTG statistics (before scaling):\n",
      "  Mean=76.23, Std=24.76, Range=[1.00, 99.34]\n",
      "‚úÖ RTG scaled by 100.0, new range: [0.01, 0.99]\n"
     ]
    }
   ],
   "source": [
    "# Convert trajectories into fixed-length context windows\n",
    "dataset = TrajectoryDataset(trajectories, context_length=20)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "# Check RTG statistics before scaling\n",
    "rtg_mean = dataset.rtg.mean()\n",
    "rtg_std = dataset.rtg.std()\n",
    "rtg_max = dataset.rtg.max()\n",
    "rtg_min = dataset.rtg.min()\n",
    "print(f\"\\nRTG statistics (before scaling):\")\n",
    "print(f\"  Mean={rtg_mean:.2f}, Std={rtg_std:.2f}, Range=[{rtg_min:.2f}, {rtg_max:.2f}]\")\n",
    "\n",
    "# === RTG SCALING ===\n",
    "# CartPole returns are in [0, 500]. We scale by 100 to get [0, 5].\n",
    "# This normalization helps the model learn more effectively.\n",
    "# IMPORTANT: Use the SAME scaling at evaluation time!\n",
    "rtg_scale_factor = 100.0\n",
    "dataset.rtg = dataset.rtg / rtg_scale_factor\n",
    "print(f\"‚úÖ RTG scaled by {rtg_scale_factor}, new range: [{dataset.rtg.min():.2f}, {dataset.rtg.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b72e531-b880-4008-9144-0e4914050d01",
   "metadata": {},
   "source": [
    "### 2. The Decision Transformer Model ‚Äì Discrete Path Integral Propagator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b628264b-6483-402d-9fa0-073be89cfb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decision Transformer: Offline RL as Sequence Modeling\n",
    "    \n",
    "    Key idea: Model (state, action, return-to-go) sequences with a Transformer,\n",
    "    then predict actions conditioned on desired returns.\n",
    "    \n",
    "    Architecture:\n",
    "        1. Embed states, actions, and RTGs into hidden_dim vectors\n",
    "        2. Add positional (timestep) embeddings\n",
    "        3. Stack as sequence: [RTG_1, s_1, a_1, RTG_2, s_2, a_2, ...]\n",
    "        4. Process with causal Transformer\n",
    "        5. Predict actions from state embeddings\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 128,\n",
    "        n_layers: int = 3,\n",
    "        n_heads: int = 8,\n",
    "        max_timestep: int = 1024,\n",
    "        dropout: float = 0.2,\n",
    "        rtg_scale: float = 5.0,  # Amplification factor for RTG embeddings\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_dim: Dimension of state observations\n",
    "            action_dim: Dimension of action space (for discrete: 1, for continuous: action_space.shape[0])\n",
    "            hidden_dim: Transformer hidden dimension\n",
    "            n_layers: Number of transformer layers\n",
    "            n_heads: Number of attention heads\n",
    "            max_timestep: Maximum timestep for positional embeddings\n",
    "            dropout: Dropout rate\n",
    "            rtg_scale: Multiplicative factor to amplify RTG signal (helps learning)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rtg_scale = rtg_scale\n",
    "\n",
    "        # CRITICAL: All embeddings should have similar complexity!\n",
    "        # RTG gets a deeper network to strengthen its conditioning signal\n",
    "        self.embed_rtg = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # State embedding - should also be deep to learn good representations\n",
    "        self.embed_state = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        \n",
    "        # Action embedding - also deep for consistency\n",
    "        self.embed_action = nn.Sequential(\n",
    "            nn.Linear(action_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "        )\n",
    "\n",
    "        # Timestep embeddings encode position in the trajectory\n",
    "        self.embed_timestep = nn.Embedding(max_timestep, hidden_dim)\n",
    "\n",
    "        # Transformer encoder with causal masking\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=4 * hidden_dim,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,  # Pre-LN for training stability\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=n_layers,\n",
    "            enable_nested_tensor=False,  # Disable for causal masking\n",
    "        )\n",
    "\n",
    "        # Action prediction head - use MLP for better expressiveness\n",
    "        self.predict_action = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, 2),  # CartPole has 2 discrete actions\n",
    "        )\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights with small random values (GPT-style)\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, states, actions, returns_to_go, timesteps, debug=False):\n",
    "        \"\"\"\n",
    "        Forward pass: predict actions given states, past actions, and desired RTG.\n",
    "        \n",
    "        Args:\n",
    "            states: (B, L, state_dim) - batch of state sequences\n",
    "            actions: (B, L, action_dim) - batch of action sequences\n",
    "            returns_to_go: (B, L, 1) - desired future returns at each timestep\n",
    "            timesteps: (B, L) - timestep indices for positional encoding\n",
    "            debug: If True, print embedding statistics\n",
    "        \n",
    "        Returns:\n",
    "            action_preds: (B, L, action_space_size) - predicted action logits\n",
    "        \n",
    "        Process:\n",
    "            1. Embed states, actions, RTG ‚Üí (B, L, hidden_dim) each\n",
    "            2. Add timestep embeddings to all\n",
    "            3. Stack as [RTG_1, s_1, a_1, RTG_2, s_2, a_2, ...] ‚Üí (B, 3L, hidden_dim)\n",
    "            4. Apply causal Transformer (can't look into future)\n",
    "            5. Extract state embeddings (every 3rd position starting at index 2)\n",
    "            6. Predict actions from state embeddings\n",
    "        \"\"\"\n",
    "        # Ensure correct dtypes\n",
    "        states = states.float()\n",
    "        actions = actions.float()\n",
    "        returns_to_go = returns_to_go.float()\n",
    "        timesteps = timesteps.long()\n",
    "\n",
    "        B, L = states.shape[:2]  # Batch size, sequence length\n",
    "\n",
    "        # === STEP 1: Embed inputs ===\n",
    "        # RTG embedding is amplified by rtg_scale to strengthen its signal\n",
    "        rtg_embed    = self.embed_rtg(returns_to_go) * self.rtg_scale\n",
    "        state_embed  = self.embed_state(states)\n",
    "        action_embed = self.embed_action(actions)\n",
    "\n",
    "        # Debug: Check embedding magnitudes\n",
    "        if debug:\n",
    "            print(f\"\\nüîç DEBUG Forward Pass:\")\n",
    "            print(f\"  RTG input range: [{returns_to_go.min():.3f}, {returns_to_go.max():.3f}]\")\n",
    "            print(f\"  RTG embed norm (before scale): {(self.embed_rtg(returns_to_go)).norm(dim=-1).mean():.3f}\")\n",
    "            print(f\"  RTG embed norm (after scale): {rtg_embed.norm(dim=-1).mean():.3f}\")\n",
    "            print(f\"  State embed norm: {state_embed.norm(dim=-1).mean():.3f}\")\n",
    "            print(f\"  Action embed norm: {action_embed.norm(dim=-1).mean():.3f}\")\n",
    "\n",
    "        # === STEP 2: Add positional (timestep) embeddings ===\n",
    "        time_embed = self.embed_timestep(timesteps)  # (B, L, hidden_dim)\n",
    "        rtg_embed    += time_embed\n",
    "        state_embed  += time_embed\n",
    "        action_embed += time_embed\n",
    "\n",
    "        # === STEP 3: Stack as interleaved sequence ===\n",
    "        # Shape: (B, L, 3, hidden_dim) ‚Üí (B, 3*L, hidden_dim)\n",
    "        # Order: [RTG_0, state_0, action_0, RTG_1, state_1, action_1, ...]\n",
    "        stacked = torch.stack((rtg_embed, state_embed, action_embed), dim=2)\n",
    "        stacked = stacked.reshape(B, 3 * L, -1)\n",
    "\n",
    "        # === STEP 4: Apply causal Transformer ===\n",
    "        # Causal mask ensures position i can only attend to positions <= i\n",
    "        # This prevents the model from \"cheating\" by seeing future information\n",
    "        seq_len = 3 * L\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(stacked.device)\n",
    "        h = self.transformer(stacked, mask=mask)  # (B, 3*L, hidden_dim)\n",
    "\n",
    "        # === STEP 5: Extract action predictions ===\n",
    "        # CRITICAL: Predict action from STATE position (1::3), not action position!\n",
    "        # Sequence order: [RTG_0, state_0, action_0, RTG_1, state_1, action_1, ...]\n",
    "        # Positions:      [ 0,      1,        2,      3,      4,        5,    ...]\n",
    "        # \n",
    "        # At position 1 (state_0), model has seen: RTG_0, state_0\n",
    "        # This is the RIGHT place to predict action_0!\n",
    "        # \n",
    "        # If we use position 2 (action_0), model would see: RTG_0, state_0, action_0\n",
    "        # This is WRONG - it's cheating by seeing the answer!\n",
    "        action_preds = self.predict_action(h[:, 1::3, :])  # (B, L, action_dim)\n",
    "\n",
    "        return action_preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2771c-8d35-4d09-b9a3-96faf75768dc",
   "metadata": {},
   "source": [
    "### 3. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99eb084-c75f-49e5-8559-66cb8b38dced",
   "metadata": {},
   "source": [
    "Initializing Decision Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c4cc4a-0fb7-4ce0-9109-b33c26ff7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTransformer(\n",
    "    state_dim=dataset.state_dim,      # 4 for CartPole (position, velocity, angle, angular_velocity)\n",
    "    action_dim=dataset.action_dim,    # 1 (we have 2 discrete actions, but store as single integer)\n",
    "    hidden_dim=128,                   \n",
    "    n_layers=4,                      \n",
    "    n_heads=4,\n",
    "    max_timestep=1024,                 # Maximum trajectory length\n",
    "    dropout=0.1,                       # Dropout for regularization\n",
    "    rtg_scale=20.0,                    \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9ee11c-5417-4482-b3cc-09bbbffd0145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10\n",
      "Total steps: 18,590\n"
     ]
    }
   ],
   "source": [
    "# Split into train/val for monitoring overfitting\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# Optimizer: AdamW with small learning rate and weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "# Training hyperparameters\n",
    "num_epochs = 10\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = num_epochs * steps_per_epoch\n",
    "\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Total steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22a79a-bf6e-4389-92f4-aa9e19dcf839",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92bef7b1-c162-4bbf-a99f-6d148cc6257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2320251cd054f4085c22b93bc6ca0d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/18590 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üîç FIRST BATCH ANALYSIS\n",
      "============================================================\n",
      "Batch shapes: states=torch.Size([256, 20, 4]), actions=torch.Size([256, 20, 1]), rtgs=torch.Size([256, 20, 1])\n",
      "RTG range in batch: [0.010, 0.992]\n",
      "Actions in batch: unique values = [0, 1]\n",
      "\n",
      "RTG-Action correlation check:\n",
      "  Low RTG (<-1): actions = []\n",
      "  High RTG (>1): actions = []\n",
      "\n",
      "üîç DEBUG Forward Pass:\n",
      "  RTG input range: [0.010, 0.992]\n",
      "  RTG embed norm (before scale): 1.632\n",
      "  RTG embed norm (after scale): 32.649\n",
      "  State embed norm: 1.454\n",
      "  Action embed norm: 0.868\n",
      "\n",
      "Epoch 1/10 | Avg train loss: 0.3824\n",
      "Epoch 1/10 | Avg val loss: 0.3629\n",
      "\n",
      "Epoch 2/10 | Avg train loss: 0.3561\n",
      "Epoch 2/10 | Avg val loss: 0.3574\n",
      "\n",
      "Epoch 3/10 | Avg train loss: 0.3508\n",
      "Epoch 3/10 | Avg val loss: 0.35\n",
      "\n",
      "Epoch 4/10 | Avg train loss: 0.3476\n",
      "Epoch 4/10 | Avg val loss: 0.3449\n",
      "\n",
      "Epoch 5/10 | Avg train loss: 0.3453\n",
      "Epoch 5/10 | Avg val loss: 0.3433\n",
      "\n",
      "Epoch 6/10 | Avg train loss: 0.3439\n",
      "Epoch 6/10 | Avg val loss: 0.3396\n",
      "\n",
      "Epoch 7/10 | Avg train loss: 0.3421\n",
      "Epoch 7/10 | Avg val loss: 0.3377\n",
      "\n",
      "Epoch 8/10 | Avg train loss: 0.341\n",
      "Epoch 8/10 | Avg val loss: 0.3461\n",
      "\n",
      "Epoch 9/10 | Avg train loss: 0.3398\n",
      "Epoch 9/10 | Avg val loss: 0.3386\n",
      "\n",
      "Epoch 10/10 | Avg train loss: 0.3386\n",
      "Epoch 10/10 | Avg val loss: 0.3363\n",
      "\n",
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "model.train()\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 3  # Early stopping: stop if no improvement for 3 epochs\n",
    "early_stop_counter = 0\n",
    "\n",
    "pbar = tqdm(total=total_steps, desc=\"Training\")\n",
    "\n",
    "# Debug flag to print first batch info\n",
    "first_batch_debug = True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # === Training Phase ===\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    num_train_batches = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        # Unpack batch and move to device\n",
    "        states, actions, rtgs, timesteps, mask = [x.to(device) for x in batch]\n",
    "\n",
    "        # Debug: Print first batch information\n",
    "        if first_batch_debug and epoch == 0:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üîç FIRST BATCH ANALYSIS\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Batch shapes: states={states.shape}, actions={actions.shape}, rtgs={rtgs.shape}\")\n",
    "            print(f\"RTG range in batch: [{rtgs.min():.3f}, {rtgs.max():.3f}]\")\n",
    "            print(f\"Actions in batch: unique values = {actions.unique().tolist()}\")\n",
    "            \n",
    "            # Check if there's any RTG-action correlation in the data\n",
    "            rtg_flat = rtgs.flatten().cpu().numpy()\n",
    "            action_flat = actions.flatten().cpu().numpy()\n",
    "            print(f\"\\nRTG-Action correlation check:\")\n",
    "            print(f\"  Low RTG (<-1): actions = {action_flat[rtg_flat < -1][:10]}\")\n",
    "            print(f\"  High RTG (>1): actions = {action_flat[rtg_flat > 1][:10]}\")\n",
    "            first_batch_debug = False\n",
    "\n",
    "        # Forward pass\n",
    "        pred_actions = model(\n",
    "            states=states,\n",
    "            actions=actions,\n",
    "            returns_to_go=rtgs,\n",
    "            timesteps=timesteps,\n",
    "            debug=(epoch == 0 and num_train_batches == 0),  # Debug embeddings in first batch\n",
    "        )\n",
    "\n",
    "        # === Compute Loss ===\n",
    "        # For discrete actions: use cross-entropy loss\n",
    "        actions_discrete = actions.squeeze(-1).long()  # (B, L) - squeeze out action_dim\n",
    "        loss = F.cross_entropy(\n",
    "            pred_actions.transpose(1, 2),  # (B, num_actions, L) - cross_entropy expects class dim at index 1\n",
    "            actions_discrete,              # (B, L) - target class indices\n",
    "            reduction='none'               # Don't reduce yet, we need to apply mask\n",
    "        )\n",
    "        # Apply mask to ignore padded positions\n",
    "        loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "        # Backward pass and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping for stability\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "\n",
    "        pbar.update(1)\n",
    "        global_step += 1\n",
    "\n",
    "        # Log progress every 1000 steps\n",
    "        if global_step % 1000 == 0:\n",
    "            pbar.set_postfix({\"train_loss\": f\"{loss.item():.4g}\"})\n",
    "\n",
    "    avg_train_loss = train_loss / num_train_batches\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} | Avg train loss: {avg_train_loss:.4g}\")\n",
    "\n",
    "    # === Validation Phase ===\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            states, actions, rtgs, timesteps, mask = [x.to(device) for x in batch]\n",
    "\n",
    "            pred_actions = model(\n",
    "                states=states,\n",
    "                actions=actions,\n",
    "                returns_to_go=rtgs,\n",
    "                timesteps=timesteps,\n",
    "            )\n",
    "\n",
    "            actions_discrete = actions.squeeze(-1).long()\n",
    "            loss = F.cross_entropy(\n",
    "                pred_actions.transpose(1, 2),\n",
    "                actions_discrete,\n",
    "                reduction='none'\n",
    "            )\n",
    "            loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "\n",
    "    avg_val_loss = val_loss / num_val_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Avg val loss: {avg_val_loss:.4g}\")\n",
    "\n",
    "    # === Early Stopping Check ===\n",
    "    if avg_val_loss < best_val_loss - 1e-5:\n",
    "        best_val_loss = avg_val_loss\n",
    "        early_stop_counter = 0\n",
    "        # Save best model checkpoint\n",
    "        torch.save(model.state_dict(), 'best_dt_model.pt')\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"‚ö†Ô∏è  Early stopping triggered after epoch {epoch+1} (no improvement for {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "pbar.close()\n",
    "print(\"\\n‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea946b-f63d-4694-8030-bfae655c592a",
   "metadata": {},
   "source": [
    "### 4. Evaluation Function\n",
    "\n",
    "The model is trained on random + expert trajectories, it is expected to achieve expert-level performance by conditioning on high returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33583338-4f7b-4f24-8d7d-79773fc9fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_dt(\n",
    "    model, \n",
    "    env_name: str,\n",
    "    target_return: float, \n",
    "    rtg_scale_factor: float,\n",
    "    context_length: int,\n",
    "    device: str,\n",
    "    n_eval: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the trained Decision Transformer in the environment.\n",
    "    \n",
    "    This is the KEY test: given a desired return (target_return), can the model\n",
    "    generate a trajectory that achieves it?\n",
    "    \n",
    "    Args:\n",
    "        model: Trained DecisionTransformer\n",
    "        env_name: Environment to evaluate in\n",
    "        target_return: Desired cumulative return (the \"goal\" we condition on)\n",
    "        rtg_scale_factor: Same scaling factor used in training\n",
    "        context_length: Maximum history length to condition on\n",
    "        device: torch device\n",
    "        n_eval: Number of evaluation episodes\n",
    "    \n",
    "    Returns:\n",
    "        (mean_return, std_return): Average and std of achieved returns\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    env = gym.make(env_name)\n",
    "    scores = []\n",
    "\n",
    "    for _ in range(n_eval):\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Maintain history of (state, action, RTG, timestep) for context window\n",
    "        states_hist = []\n",
    "        actions_hist = []\n",
    "        rtgs_hist = []\n",
    "        timesteps_hist = []\n",
    "\n",
    "        # Scale target return same way as training data\n",
    "        norm_rtg = target_return / rtg_scale_factor\n",
    "        episode_reward = 0.0\n",
    "        done = False\n",
    "        timestep = 0\n",
    "\n",
    "        gamma = 0.99  # Must match training\n",
    "\n",
    "        while not done:\n",
    "            # === Build input for prediction ===\n",
    "            # Add current state, rtg, and timestep\n",
    "            states_hist.append(state.unsqueeze(0))  # (1, state_dim)\n",
    "            rtgs_hist.append(torch.tensor([norm_rtg], dtype=torch.float32, device=device))\n",
    "            timesteps_hist.append(torch.tensor([timestep], dtype=torch.long, device=device))\n",
    "            \n",
    "            # For actions: use dummy action for the current timestep (we're predicting it)\n",
    "            # and real actions for all previous timesteps\n",
    "            actions_in_list = actions_hist.copy() + [torch.zeros(1, 1, dtype=torch.long, device=device)]\n",
    "            \n",
    "            # Build context window (last context_length steps)\n",
    "            hist_len = len(states_hist)\n",
    "            start_idx = max(0, hist_len - context_length)\n",
    "            \n",
    "            # Concatenate history into model input format\n",
    "            states_in = torch.cat(states_hist[start_idx:], dim=0).unsqueeze(0)      # (1, T, state_dim)\n",
    "            actions_in = torch.cat(actions_in_list[start_idx:], dim=0).unsqueeze(0)  # (1, T, 1)\n",
    "            rtgs_in = torch.cat(rtgs_hist[start_idx:], dim=0).unsqueeze(0).unsqueeze(-1)  # (1, T, 1)\n",
    "            timesteps_in = torch.cat(timesteps_hist[start_idx:], dim=0).unsqueeze(0)  # (1, T)\n",
    "\n",
    "            # === Left-pad if history shorter than context_length ===\n",
    "            if hist_len < context_length:\n",
    "                pad_len = context_length - hist_len\n",
    "                # Pad with zeros (model will learn to ignore padded positions)\n",
    "                pad_states = torch.zeros(1, pad_len, states_in.shape[-1], device=device)\n",
    "                pad_actions = torch.zeros(1, pad_len, 1, dtype=torch.long, device=device)\n",
    "                pad_rtgs = torch.zeros(1, pad_len, 1, device=device)\n",
    "                pad_times = torch.zeros(1, pad_len, dtype=torch.long, device=device)\n",
    "\n",
    "                states_in = torch.cat([pad_states, states_in], dim=1)\n",
    "                actions_in = torch.cat([pad_actions, actions_in], dim=1)\n",
    "                rtgs_in = torch.cat([pad_rtgs, rtgs_in], dim=1)\n",
    "                timesteps_in = torch.cat([pad_times, timesteps_in], dim=1)\n",
    "\n",
    "            # === Predict action ===\n",
    "            with torch.no_grad():\n",
    "                # Debug first step\n",
    "                if timestep == 0:\n",
    "                    print(f\"\\nüîç EVAL DEBUG (timestep=0):\")\n",
    "                    print(f\"  states_in.shape: {states_in.shape}\")\n",
    "                    print(f\"  actions_in.shape: {actions_in.shape}\")\n",
    "                    print(f\"  rtgs_in.shape: {rtgs_in.shape}\")\n",
    "                    print(f\"  rtgs_in value: {rtgs_in[0, -1, 0].item():.3f} (normalized)\")\n",
    "                \n",
    "                pred = model(states=states_in, actions=actions_in, returns_to_go=rtgs_in, timesteps=timesteps_in)\n",
    "                action_logit = pred[0, -1]  # Take last timestep's prediction\n",
    "                action_to_take = torch.argmax(action_logit).item()  # Greedy action selection\n",
    "                \n",
    "                if timestep == 0:\n",
    "                    print(f\"  pred.shape: {pred.shape}\")\n",
    "                    print(f\"  action_logit: {action_logit}\")\n",
    "                    print(f\"  selected action: {action_to_take}\")\n",
    "\n",
    "            # === Execute action in environment ===\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_to_take)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "            # === Update for next step ===\n",
    "            # Store the action we just took (for next prediction's history)\n",
    "            actions_hist.append(torch.tensor([[action_to_take]], dtype=torch.long, device=device))\n",
    "            \n",
    "            # Update state\n",
    "            state = torch.tensor(next_state, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # Update RTG with gamma to match training\n",
    "            # Training: rtg[t] = r[t] + gamma * rtg[t+1]\n",
    "            # Inference: rtg[t+1] = (rtg[t] - r[t]) / gamma\n",
    "            norm_rtg = (norm_rtg - reward / rtg_scale_factor) / gamma\n",
    "            \n",
    "            timestep += 1\n",
    "\n",
    "        scores.append(episode_reward)\n",
    "\n",
    "    env.close()\n",
    "    mean = np.mean(scores)\n",
    "    std = np.std(scores)\n",
    "    return mean, std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09290420-4da5-41c8-b5f7-35bda7d384dd",
   "metadata": {},
   "source": [
    "### 5. DEBUG & ANALYSIS - Test RTG Conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8bbaec2-2a9c-4120-9316-335ba97d1ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTransformer(\n",
       "  (embed_rtg): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (embed_state): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (embed_action): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (embed_timestep): Embedding(1024, 128)\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predict_action): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c8d3a4-daa6-4778-9d2f-3ca3c1fef2f7",
   "metadata": {},
   "source": [
    "### 6. EVALUATION - Test in Real Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00383a29-a4c4-480a-8079-fabc5281cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data statistics:\n",
      "  Mean return: 316.2 ¬± 184.5\n",
      "  Return range: [9, 500]\n",
      "  Target RTG range: [9, 500]\n",
      "  Correlation (target_rtg vs actual_return): 1.000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data statistics:\")\n",
    "all_returns = [t['returns'] for t in trajectories]\n",
    "all_target_rtgs = [t['target_rtg'] for t in trajectories]\n",
    "print(f\"  Mean return: {np.mean(all_returns):.1f} ¬± {np.std(all_returns):.1f}\")\n",
    "print(f\"  Return range: [{np.min(all_returns):.0f}, {np.max(all_returns):.0f}]\")\n",
    "print(f\"  Target RTG range: [{np.min(all_target_rtgs):.0f}, {np.max(all_target_rtgs):.0f}]\")\n",
    "print(f\"  Correlation (target_rtg vs actual_return): {np.corrcoef(all_target_rtgs, all_returns)[0,1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249a6ba4-f56b-479e-956b-b1351facecb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test 1: Low Target RTG\n",
    "\n",
    "Ask for poor performance - should match random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81a04f8d-9dc6-4a71-8ebf-2a5921c6ffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5866, -0.5397], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2815,  0.1674], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4603,  0.1679], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5743, -0.5328], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4584,  0.1638], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3924,  0.0960], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6003, -0.5624], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6224, -0.5733], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3165,  0.0210], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.0682, -0.0389], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2808,  0.1751], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5943, -0.5383], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4530,  0.1515], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6090, -0.5583], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5860, -0.5395], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6170, -0.5875], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6253, -0.5909], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6277, -0.5897], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1209, -0.1340], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.0868, -0.0293], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6042, -0.5481], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4576,  0.1625], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5995, -0.5449], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4617,  0.1695], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5973, -0.5423], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6054, -0.5509], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6181, -0.5737], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4492,  0.1425], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6078, -0.5542], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4522,  0.1513], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4433,  0.1372], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4579,  0.1561], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5820, -0.5537], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4585,  0.1578], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1023, -0.1623], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2622,  0.1485], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4604,  0.1632], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4623,  0.1689], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2527,  0.1423], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1264, -0.1485], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.2649, -0.3004], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6168, -0.5663], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2120,  0.0780], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.0320, -0.0790], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.6237, -0.5820], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2169, -0.0711], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4513,  0.1467], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3959,  0.0797], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.5576, -0.5159], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 0.500 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4070,  0.1067], device='mps:0')\n",
      "  selected action: 1\n",
      "   Target RTG=50  ‚Üí Achieved: 9.5 ¬± 1.0\n"
     ]
    }
   ],
   "source": [
    "low_target = 50\n",
    "low_mean, low_std = evaluate_dt(\n",
    "    model=model, env_name=env_name, target_return=low_target,\n",
    "    rtg_scale_factor=rtg_scale_factor,\n",
    "    context_length=dataset.context_length,\n",
    "    device=device, n_eval=50\n",
    ")\n",
    "print(f\"   Target RTG={low_target}  ‚Üí Achieved: {low_mean:.1f} ¬± {low_std:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83db908-4c3d-4ef0-ab43-8c98d0de1686",
   "metadata": {},
   "source": [
    "#### Test 2: High Target RTG\n",
    "\n",
    "Ask for expert performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5b28f0b-fd15-4493-baff-9b261216e2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Test 2: HIGH target RTG (asking for expert performance)\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3681,  0.1792], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.0930, -0.3420], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.1139, -0.1562], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.5354,  0.3193], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.5169,  0.3416], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2208, -0.0834], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1591, -0.1659], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.7416,  0.5185], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3319,  0.0272], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3412,  0.0379], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.0653, -0.2344], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1557,  0.0697], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.9728,  0.7119], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2900, -0.0350], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2264, -0.0848], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.6457,  0.4185], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.0280, -0.2706], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.0698, -0.3396], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2896,  0.1426], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.0278, -0.2698], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.2484, -0.2608], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-1.1090,  0.8848], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.7671,  0.5626], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-1.1192,  0.9014], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.0819, -0.1663], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3348,  0.0272], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3007, -0.0211], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.1594, -0.1912], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.2797, -0.2906], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3384,  0.0345], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1681, -0.1332], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.2418, -0.2639], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.2398, -0.2591], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2618, -0.0530], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1662, -0.1377], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.3264, -0.3526], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.4460,  0.3090], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-1.0275,  0.7899], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3403,  0.0354], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3306,  0.0207], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.8230,  0.5721], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-1.0449,  0.8274], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2910, -0.0273], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.0837, -0.2312], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.1891, -0.2286], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3268,  0.0215], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.1002, -0.1938], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.2878, -0.0355], device='mps:0')\n",
      "  selected action: 1\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([ 0.0944, -0.1722], device='mps:0')\n",
      "  selected action: 0\n",
      "\n",
      "üîç EVAL DEBUG (timestep=0):\n",
      "  states_in.shape: torch.Size([1, 20, 4])\n",
      "  actions_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in.shape: torch.Size([1, 20, 1])\n",
      "  rtgs_in value: 5.000 (normalized)\n",
      "  pred.shape: torch.Size([1, 20, 2])\n",
      "  action_logit: tensor([-0.3447,  0.0395], device='mps:0')\n",
      "  selected action: 1\n",
      "   Target RTG=500 ‚Üí Achieved: 500.0 ¬± 0.0\n"
     ]
    }
   ],
   "source": [
    "high_target = 500\n",
    "high_mean, high_std = evaluate_dt(\n",
    "    model=model, env_name=env_name, target_return=high_target,\n",
    "    rtg_scale_factor=rtg_scale_factor,\n",
    "    context_length=dataset.context_length,\n",
    "    device=device, n_eval=50\n",
    ")\n",
    "print(f\"   Target RTG={high_target} ‚Üí Achieved: {high_mean:.1f} ¬± {high_std:.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297e9cb-a669-4bc5-a81e-0043614351cd",
   "metadata": {},
   "source": [
    "DT learned to reproduce expert behavior from mixed offline data!\n",
    "By conditioning on high RTG (500), it achieves expert-level performance.\n",
    "This demonstrates offline RL working: no environment interaction during training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhysAI-Zen (MPS)",
   "language": "python",
   "name": "physai-zen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
