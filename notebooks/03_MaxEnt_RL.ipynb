{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 03: Maximum-Entropy RL — Langevin Dynamics + Maximum Caliber\n",
    "\n",
    "> \"SAC is not inspired by physics — it is physics applied to control.\"\n",
    "\n",
    "This notebook demonstrates the deep connection between statistical physics and reinforcement learning through:\n",
    "1. **Langevin Dynamics**: Stochastic exploration in energy landscapes\n",
    "2. **Boltzmann Policies**: The optimal policy form from MaxEnt principle\n",
    "3. **Soft Actor-Critic (SAC)**: State-of-the-art MaxEnt (Maximum Entropy) RL algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from collections import deque\n",
    "import random\n",
    "import gymnasium as gym\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Plot styling\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Langevin Dynamics — Exploring Energy Landscapes\n",
    "\n",
    "### Theory\n",
    "\n",
    "Langevin dynamics describes a particle in a potential $V(x)$ with thermal fluctuations:\n",
    "\n",
    "$$\n",
    "\\dot{x} = -\\nabla V(x) + \\sqrt{2D} \\, \\eta(t)\n",
    "$$\n",
    "\n",
    "The stationary distribution is the Boltzmann distribution:\n",
    "\n",
    "$$\n",
    "p_{\\text{eq}}(x) \\propto \\exp\\left(-\\frac{V(x)}{k_B T}\\right)\n",
    "$$\n",
    "\n",
    "**Temperature controls exploration**: High $T$ → wide exploration; Low $T$ → exploitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_well_potential(x):\n",
    "    \"\"\"Double-well potential: V(x) = (x^2 - 1)^2\"\"\"\n",
    "    return (x**2 - 1)**2\n",
    "\n",
    "def gradient_potential(x):\n",
    "    \"\"\"Gradient of double-well potential\"\"\"\n",
    "    return 4 * x * (x**2 - 1)\n",
    "\n",
    "def langevin_dynamics(V_grad, x0, T, dt=0.01, steps=10000):\n",
    "    \"\"\"\n",
    "    Simulate Langevin dynamics.\n",
    "    \n",
    "    Args:\n",
    "        V_grad: Gradient of potential function\n",
    "        x0: Initial position\n",
    "        T: Temperature (controls exploration)\n",
    "        dt: Time step\n",
    "        steps: Number of steps\n",
    "    \n",
    "    Returns:\n",
    "        trajectory: Array of positions over time\n",
    "    \"\"\"\n",
    "    D = T  # Diffusion coefficient (set k_B = gamma = 1)\n",
    "    trajectory = np.zeros(steps)\n",
    "    x = x0\n",
    "    \n",
    "    for i in range(steps):\n",
    "        # Langevin update: drift + diffusion\n",
    "        drift = -V_grad(x)\n",
    "        diffusion = np.sqrt(2 * D * dt) * np.random.randn()\n",
    "        x = x + drift * dt + diffusion\n",
    "        trajectory[i] = x\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "# Simulate at different temperatures\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0]\n",
    "trajectories = {}\n",
    "\n",
    "for T in temperatures:\n",
    "    traj = langevin_dynamics(gradient_potential, x0=0.5, T=T, steps=50000)\n",
    "    trajectories[T] = traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Langevin dynamics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "x_range = np.linspace(-2, 2, 200)\n",
    "V = double_well_potential(x_range)\n",
    "\n",
    "for idx, T in enumerate(temperatures):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot potential landscape\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x_range, V, 'k-', alpha=0.3, linewidth=2, label='Potential V(x)')\n",
    "    ax2.set_ylabel('Potential V(x)', fontsize=11)\n",
    "    ax2.set_ylim(-0.5, 3)\n",
    "    \n",
    "    # Plot histogram of visited states\n",
    "    traj = trajectories[T]\n",
    "    ax.hist(traj[10000:], bins=100, density=True, alpha=0.6, color='blue', label='Empirical')\n",
    "    \n",
    "    # Theoretical Boltzmann distribution\n",
    "    Z = np.trapezoid(np.exp(-V / T), x_range)\n",
    "    p_boltzmann = np.exp(-V / T) / Z\n",
    "    ax.plot(x_range, p_boltzmann, 'r-', linewidth=2, label='Boltzmann (theory)')\n",
    "    \n",
    "    ax.set_xlabel('Position x', fontsize=11)\n",
    "    ax.set_ylabel('Probability Density', fontsize=11)\n",
    "    ax.set_title(f'Temperature T = {T}', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper left', fontsize=9)\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Langevin Dynamics: Temperature Controls Exploration', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(f\"• T={temperatures[0]}: Low temperature → trapped in one well (exploitation)\")\n",
    "print(f\"• T={temperatures[-1]}: High temperature → explores both wells (exploration)\")\n",
    "print(f\"• Empirical histograms match theoretical Boltzmann distribution!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Boltzmann Policy — The Optimal Policy Form\n",
    "\n",
    "### Theory\n",
    "\n",
    "In MaxEnt RL, the optimal policy is:\n",
    "\n",
    "$$\n",
    "\\pi^*(a|s) = \\frac{1}{Z(s)} \\exp\\left(\\frac{Q^*(s,a)}{\\alpha}\\right)\n",
    "$$\n",
    "\n",
    "This is **exactly the Boltzmann distribution** with:\n",
    "- \"Energy\" = $-Q^*(s,a)$ (negative Q-value)\n",
    "- \"Temperature\" = $\\alpha$ (entropy coefficient)\n",
    "\n",
    "**This is not a heuristic** — it's the unique solution to:\n",
    "$$\n",
    "\\max_\\pi \\, \\mathbb{E}_\\pi[Q(s, \\cdot)] + \\alpha H(\\pi)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boltzmann_policy(Q_values, alpha):\n",
    "    \"\"\"\n",
    "    Compute Boltzmann policy from Q-values.\n",
    "    \n",
    "    Args:\n",
    "        Q_values: Array of Q(s, a) for each action\n",
    "        alpha: Temperature parameter\n",
    "    \n",
    "    Returns:\n",
    "        policy: Probability distribution over actions\n",
    "    \"\"\"\n",
    "    exp_Q = np.exp(Q_values / alpha)\n",
    "    Z = np.sum(exp_Q)  # Partition function\n",
    "    return exp_Q / Z\n",
    "\n",
    "def policy_entropy(policy):\n",
    "    \"\"\"Compute entropy H(π) = -Σ π(a) log π(a)\"\"\"\n",
    "    return -np.sum(policy * np.log(policy + 1e-10))\n",
    "\n",
    "# Example: 5 actions with different Q-values\n",
    "Q_values = np.array([1.0, 3.0, 2.0, 0.5, 2.5])\n",
    "actions = np.arange(len(Q_values))\n",
    "alphas = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "\n",
    "# Compute policies at different temperatures\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    policy = boltzmann_policy(Q_values, alpha)\n",
    "    entropy = policy_entropy(policy)\n",
    "    expected_Q = np.sum(policy * Q_values)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    bars = ax.bar(actions, policy, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "    \n",
    "    # Highlight best action\n",
    "    best_action = np.argmax(Q_values)\n",
    "    bars[best_action].set_color('crimson')\n",
    "    bars[best_action].set_alpha(0.9)\n",
    "    \n",
    "    ax.set_xlabel('Action', fontsize=11)\n",
    "    ax.set_ylabel('Probability π(a|s)', fontsize=11)\n",
    "    ax.set_title(f'α = {alpha}\\nH(π) = {entropy:.2f}, E[Q] = {expected_Q:.2f}', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add Q-values as text\n",
    "    for i, (a, q) in enumerate(zip(actions, Q_values)):\n",
    "        ax.text(a, -0.08, f'Q={q:.1f}', ha='center', fontsize=9, color='gray')\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[-1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Boltzmann Policy: Temperature Controls Exploration-Exploitation', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(f\"• α→0: Policy becomes deterministic (picks best action, red bar)\")\n",
    "print(f\"• α→∞: Policy becomes uniform (maximum exploration)\")\n",
    "print(f\"• Intermediate α: Balances exploitation (high Q) and exploration (entropy)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Soft Actor-Critic (SAC) Implementation\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "SAC maintains:\n",
    "1. **Soft Q-networks**: $Q_\\phi(s, a)$ (two networks, take minimum)\n",
    "2. **Policy network**: $\\pi_\\theta(a|s)$ (Gaussian for continuous actions)\n",
    "3. **Target networks**: $Q_{\\phi'}$ (slowly updated)\n",
    "\n",
    "**Key equations**:\n",
    "- Soft Bellman target: $y = r + \\gamma (Q_{\\text{min}}(s', a') - \\alpha \\log \\pi(a'|s'))$\n",
    "- Policy loss: $L_\\pi = \\mathbb{E}[\\alpha \\log \\pi(a|s) - Q(s, a)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architectures\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Soft Q-function approximator.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class GaussianPolicy(nn.Module):\n",
    "    \"\"\"Squashed Gaussian policy for continuous actions.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, log_std_min=-20, log_std_max=2):\n",
    "        super().__init__()\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "        \n",
    "        self.trunk = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mean_head = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std_head = nn.Linear(hidden_dim, action_dim)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.trunk(state)\n",
    "        mean = self.mean_head(x)\n",
    "        log_std = self.log_std_head(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "    \n",
    "    def sample(self, state, deterministic=False):\n",
    "        \"\"\"\n",
    "        Sample action from policy.\n",
    "        \n",
    "        Returns:\n",
    "            action: Sampled action (squashed to [-1, 1])\n",
    "            log_prob: Log probability of the action\n",
    "        \"\"\"\n",
    "        mean, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        \n",
    "        if deterministic:\n",
    "            # Use mean for evaluation\n",
    "            action = torch.tanh(mean)\n",
    "            return action, None\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        normal = Normal(mean, std)\n",
    "        x = normal.rsample()  # Sample with reparameterization\n",
    "        action = torch.tanh(x)\n",
    "        \n",
    "        # Compute log probability with tanh correction\n",
    "        log_prob = normal.log_prob(x)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer.\"\"\"\n",
    "    def __init__(self, capacity=1000000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.from_numpy(np.array(states)).float(),\n",
    "            torch.from_numpy(np.array(actions)).float(),\n",
    "            torch.from_numpy(np.array(rewards)).float().unsqueeze(1),\n",
    "            torch.from_numpy(np.array(next_states)).float(),\n",
    "            torch.from_numpy(np.array(dones)).float().unsqueeze(1)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \"\"\"Soft Actor-Critic agent.\"\"\"\n",
    "    def __init__(self, state_dim, action_dim, alpha=0.2, gamma=0.99, tau=0.005, lr=3e-4):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # Networks\n",
    "        self.q1 = QNetwork(state_dim, action_dim)\n",
    "        self.q2 = QNetwork(state_dim, action_dim)\n",
    "        self.q1_target = QNetwork(state_dim, action_dim)\n",
    "        self.q2_target = QNetwork(state_dim, action_dim)\n",
    "        \n",
    "        # Copy parameters to targets\n",
    "        self.q1_target.load_state_dict(self.q1.state_dict())\n",
    "        self.q2_target.load_state_dict(self.q2.state_dict())\n",
    "        \n",
    "        self.policy = GaussianPolicy(state_dim, action_dim)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.q_optimizer = torch.optim.Adam(\n",
    "            list(self.q1.parameters()) + list(self.q2.parameters()), lr=lr\n",
    "        )\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        # Metrics\n",
    "        self.q_losses = []\n",
    "        self.policy_losses = []\n",
    "        self.entropies = []\n",
    "    \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        \"\"\"Select action from policy.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action, _ = self.policy.sample(state, deterministic=deterministic)\n",
    "        return action.squeeze(0).numpy()\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update networks with a batch of experiences.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        # --- Critic Update ---\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs = self.policy.sample(next_states)\n",
    "            q1_next = self.q1_target(next_states, next_actions)\n",
    "            q2_next = self.q2_target(next_states, next_actions)\n",
    "            q_next = torch.min(q1_next, q2_next)\n",
    "            \n",
    "            # Soft Bellman target\n",
    "            target = rewards + self.gamma * (1 - dones) * (q_next - self.alpha * next_log_probs)\n",
    "        \n",
    "        q1_pred = self.q1(states, actions)\n",
    "        q2_pred = self.q2(states, actions)\n",
    "        \n",
    "        q1_loss = F.mse_loss(q1_pred, target)\n",
    "        q2_loss = F.mse_loss(q2_pred, target)\n",
    "        q_loss = q1_loss + q2_loss\n",
    "        \n",
    "        self.q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "        \n",
    "        # --- Actor Update ---\n",
    "        new_actions, log_probs = self.policy.sample(states)\n",
    "        q1_new = self.q1(states, new_actions)\n",
    "        q2_new = self.q2(states, new_actions)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "        \n",
    "        policy_loss = (self.alpha * log_probs - q_new).mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        # --- Soft Update Target Networks ---\n",
    "        self._soft_update(self.q1, self.q1_target)\n",
    "        self._soft_update(self.q2, self.q2_target)\n",
    "        \n",
    "        # Record metrics\n",
    "        self.q_losses.append(q_loss.item())\n",
    "        self.policy_losses.append(policy_loss.item())\n",
    "        self.entropies.append(-log_probs.mean().item())\n",
    "    \n",
    "    def _soft_update(self, source, target):\n",
    "        \"\"\"Polyak averaging: θ' ← τθ + (1-τ)θ'\"\"\"\n",
    "        for param, target_param in zip(source.parameters(), target.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                self.tau * param.data + (1 - self.tau) * target_param.data\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"✓ SAC agent implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training on Pendulum Environment\n",
    "\n",
    "We'll train SAC on the classic **Pendulum-v1** environment from Gymnasium:\n",
    "- **State**: (cos(θ), sin(θ), angular velocity)\n",
    "- **Action**: Torque [-2, 2]\n",
    "- **Goal**: Swing up and balance the pendulum\n",
    "\n",
    "This demonstrates MaxEnt RL on a continuous control task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac(env_name='Pendulum-v1', num_episodes=200, batch_size=256, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Train SAC on a Gymnasium environment.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the Gymnasium environment\n",
    "        num_episodes: Number of training episodes\n",
    "        batch_size: Batch size for updates\n",
    "        alpha: Temperature parameter (entropy coefficient)\n",
    "    \n",
    "    Returns:\n",
    "        agent: Trained SAC agent\n",
    "        rewards: List of episode rewards\n",
    "        entropies: List of average entropies per episode\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    \n",
    "    agent = SAC(state_dim, action_dim, alpha=alpha)\n",
    "    replay_buffer = ReplayBuffer()\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_entropies = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_entropy = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action\n",
    "            action = agent.select_action(state)\n",
    "            \n",
    "            # Take step in environment\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store transition\n",
    "            replay_buffer.push(state, action, reward, next_state, float(done))\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            # Update agent\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch = replay_buffer.sample(batch_size)\n",
    "                agent.update(batch)\n",
    "                if agent.entropies:\n",
    "                    episode_entropy.append(agent.entropies[-1])\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_entropies.append(np.mean(episode_entropy) if episode_entropy else 0)\n",
    "        \n",
    "        if (episode + 1) % 20 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-20:])\n",
    "            avg_entropy = np.mean(episode_entropies[-20:])\n",
    "            print(f\"Episode {episode+1}/{num_episodes} | \"\n",
    "                  f\"Avg Reward: {avg_reward:.2f} | \"\n",
    "                  f\"Avg Entropy: {avg_entropy:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards, episode_entropies\n",
    "\n",
    "\n",
    "# Train SAC\n",
    "print(\"Training SAC on Pendulum-v1...\\n\")\n",
    "agent, rewards, entropies = train_sac(num_episodes=200, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training progress\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Episode rewards\n",
    "ax = axes[0]\n",
    "ax.plot(rewards, alpha=0.3, color='blue')\n",
    "window = 10\n",
    "smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "ax.plot(range(window-1, len(rewards)), smoothed, color='darkblue', linewidth=2, label='Smoothed')\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Episode Reward', fontsize=11)\n",
    "ax.set_title('Learning Curve', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Policy entropy\n",
    "ax = axes[1]\n",
    "ax.plot(entropies, alpha=0.6, color='orange', linewidth=1.5)\n",
    "ax.axhline(y=np.mean(entropies), color='red', linestyle='--', linewidth=2, label='Mean')\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Policy Entropy H(π)', fontsize=11)\n",
    "ax.set_title('Exploration (Entropy Over Time)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# Q-loss and policy loss\n",
    "ax = axes[2]\n",
    "if agent.q_losses:\n",
    "    window = 100\n",
    "    q_smoothed = np.convolve(agent.q_losses, np.ones(window)/window, mode='valid')\n",
    "    p_smoothed = np.convolve(agent.policy_losses, np.ones(window)/window, mode='valid')\n",
    "    \n",
    "    ax.plot(q_smoothed, color='green', linewidth=2, label='Q-loss')\n",
    "    ax.plot(p_smoothed, color='purple', linewidth=2, label='Policy loss')\n",
    "    ax.set_xlabel('Update Step', fontsize=11)\n",
    "    ax.set_ylabel('Loss', fontsize=11)\n",
    "    ax.set_title('Training Losses', fontsize=12, fontweight='bold')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"• Average reward (last 20 episodes): {np.mean(rewards[-20:]):.2f}\")\n",
    "print(f\"• Average entropy (last 20 episodes): {np.mean(entropies[-20:]):.2f}\")\n",
    "print(f\"• Policy maintains exploration throughout training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Temperature Ablation Study\n",
    "\n",
    "Let's empirically verify that the temperature parameter $\\alpha$ controls the exploration-exploitation tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SAC with different temperatures\n",
    "alphas = [0.05, 0.1, 0.2, 0.5]\n",
    "results = {}\n",
    "\n",
    "print(\"Running temperature ablation study...\\n\")\n",
    "for alpha in alphas:\n",
    "    print(f\"Training with α = {alpha}...\")\n",
    "    agent_i, rewards_i, entropies_i = train_sac(\n",
    "        num_episodes=500, \n",
    "        alpha=alpha, \n",
    "        batch_size=256\n",
    "    )\n",
    "    results[alpha] = {\n",
    "        'agent': agent_i,\n",
    "        'rewards': rewards_i,\n",
    "        'entropies': entropies_i\n",
    "    }\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temperature effects\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "window = 10\n",
    "\n",
    "# Rewards\n",
    "ax = axes[0]\n",
    "for (alpha, data), color in zip(results.items(), colors):\n",
    "    rewards = data['rewards']\n",
    "    smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
    "    ax.plot(range(window-1, len(rewards)), smoothed, \n",
    "            color=color, linewidth=2, label=f'α = {alpha}')\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Episode Reward (smoothed)', fontsize=11)\n",
    "ax.set_title('Performance vs Temperature', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Entropies\n",
    "ax = axes[1]\n",
    "for (alpha, data), color in zip(results.items(), colors):\n",
    "    entropies = data['entropies']\n",
    "    ax.plot(entropies, color=color, linewidth=2, alpha=0.7, label=f'α = {alpha}')\n",
    "\n",
    "ax.set_xlabel('Episode', fontsize=11)\n",
    "ax.set_ylabel('Policy Entropy H(π)', fontsize=11)\n",
    "ax.set_title('Exploration vs Temperature', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"• Higher α → Higher entropy (more exploration)\")\n",
    "print(\"• Lower α → Faster convergence but risk of local optima\")\n",
    "print(\"• Optimal α balances exploration and exploitation\")\n",
    "print(f\"\\nFinal rewards (last 20 episodes):\")\n",
    "for alpha, data in results.items():\n",
    "    final_reward = np.mean(data['rewards'][-20:])\n",
    "    print(f\"  α = {alpha}: {final_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Visualizing the Trained Policy\n",
    "\n",
    "Let's visualize how the trained policy behaves in the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env_name='Pendulum-v1', num_episodes=5, render=False):\n",
    "    \"\"\"Evaluate trained policy.\"\"\"\n",
    "    env = gym.make(env_name, render_mode='rgb_array' if render else None)\n",
    "    rewards = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state, deterministic=True)\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        print(f\"Episode {ep+1}: Reward = {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"\\nAverage reward: {np.mean(rewards):.2f} ± {np.std(rewards):.2f}\")\n",
    "    return rewards\n",
    "\n",
    "# Evaluate best agent\n",
    "print(\"Evaluating trained policy...\\n\")\n",
    "eval_rewards = evaluate_policy(agent, num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize policy and Q-function in state space\n",
    "def visualize_policy_and_Q(agent, resolution=50):\n",
    "    \"\"\"Visualize policy actions and Q-values over state space.\"\"\"\n",
    "    # Sample states: vary angle and angular velocity\n",
    "    angles = np.linspace(-np.pi, np.pi, resolution)\n",
    "    velocities = np.linspace(-8, 8, resolution)\n",
    "    \n",
    "    actions_grid = np.zeros((resolution, resolution))\n",
    "    q_values_grid = np.zeros((resolution, resolution))\n",
    "    \n",
    "    for i, theta in enumerate(angles):\n",
    "        for j, vel in enumerate(velocities):\n",
    "            state = np.array([np.cos(theta), np.sin(theta), vel])\n",
    "            \n",
    "            # Get action\n",
    "            action = agent.select_action(state, deterministic=True)\n",
    "            actions_grid[j, i] = action[0]\n",
    "            \n",
    "            # Get Q-value\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "                action_t = torch.FloatTensor(action).unsqueeze(0)\n",
    "                q1 = agent.q1(state_t, action_t).item()\n",
    "                q2 = agent.q2(state_t, action_t).item()\n",
    "                q_values_grid[j, i] = min(q1, q2)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Policy actions\n",
    "    ax = axes[0]\n",
    "    im1 = ax.imshow(actions_grid, extent=[-np.pi, np.pi, -8, 8], \n",
    "                    aspect='auto', origin='lower', cmap='RdBu_r')\n",
    "    ax.set_xlabel('Angle θ (rad)', fontsize=11)\n",
    "    ax.set_ylabel('Angular Velocity (rad/s)', fontsize=11)\n",
    "    ax.set_title('Policy Actions π(s)', fontsize=12, fontweight='bold')\n",
    "    ax.axvline(x=0, color='black', linestyle='--', alpha=0.5, label='Upright')\n",
    "    plt.colorbar(im1, ax=ax, label='Torque')\n",
    "    \n",
    "    # Q-values\n",
    "    ax = axes[1]\n",
    "    im2 = ax.imshow(q_values_grid, extent=[-np.pi, np.pi, -8, 8], \n",
    "                    aspect='auto', origin='lower', cmap='viridis')\n",
    "    ax.set_xlabel('Angle θ (rad)', fontsize=11)\n",
    "    ax.set_ylabel('Angular Velocity (rad/s)', fontsize=11)\n",
    "    ax.set_title('Q-Values Q(s, π(s))', fontsize=12, fontweight='bold')\n",
    "    ax.axvline(x=0, color='white', linestyle='--', alpha=0.5, label='Upright')\n",
    "    plt.colorbar(im2, ax=ax, label='Q-value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Interpretation:\")\n",
    "    print(\"• Left plot: Policy applies torque to swing up and stabilize\")\n",
    "    print(\"• Right plot: Q-values are highest near upright position (θ=0)\")\n",
    "    print(\"• Policy learns smooth, continuous control strategy\")\n",
    "\n",
    "visualize_policy_and_Q(agent, resolution=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
