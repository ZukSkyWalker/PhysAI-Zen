{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5f9663f-056f-48ad-9702-fa8e6a6d6868",
   "metadata": {},
   "source": [
    "# Chapter 01: IsingGPT – Transformer Learns Phase Transitions\n",
    "\n",
    "\n",
    "Let's train a Transformer on equilibrium samples from the 1D Ising model spontaneously discovers the Boltzmann distribution, nearest-neighbor spin correlations, and phase-transition behavior — without ever seeing the Hamiltonian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfcd8c-313e-4dc2-967d-4d62ffafa2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import physai\n",
    "\n",
    "plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "plt.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"notebook\", font_scale=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2d093-5584-4901-a828-4e6710236ccf",
   "metadata": {},
   "source": [
    "### 1. Generate the data\n",
    "\n",
    "We generate $N$ samples, each sample is a spin chain of length $L$. Then the spin direction {-1, +1} can be represented by a token (0/1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392784e-8c6e-4ac0-ac9d-a305e9b0772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15_000\n",
    "\n",
    "low_T  = physai.generate_ising_samples(temp=0.5, n_samples=N, equilibration_steps=900)\n",
    "mid_T  = physai.generate_ising_samples(temp=1.0, n_samples=N, equilibration_steps=900)\n",
    "high_T = physai.generate_ising_samples(temp=3.0, n_samples=N, equilibration_steps=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3269ff-b5fe-4be8-81a4-8bab9ac5dd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_chains(samples: torch.Tensor, title: str):\n",
    "    # Move to CPU + convert to numpy (fixes the MPS error)\n",
    "    data = samples[:10].cpu().numpy()          # show 10 chains for better visual\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Beautiful physics-style colormap: red = +1 (up), blue = -1 (down)\n",
    "    im = plt.imshow(\n",
    "        data,\n",
    "        cmap='RdBu_r',\n",
    "        aspect='auto',\n",
    "        vmin=-1, vmax=1,\n",
    "        interpolation='nearest'\n",
    "    )\n",
    "    \n",
    "    plt.title(title, fontsize=18, pad=20)\n",
    "    plt.xlabel('Site index (position along chain)', fontsize=14)\n",
    "    plt.ylabel('Sample index', fontsize=14)\n",
    "    \n",
    "    # Colorbar with physical meaning\n",
    "    cbar = plt.colorbar(im, ticks=[-1, 0, 1], shrink=0.8)\n",
    "    cbar.ax.set_yticklabels(['↓ (-1)', '0', '↑ (+1)'])\n",
    "    cbar.set_label('Spin', rotation=270, labelpad=20, fontsize=14)\n",
    "    \n",
    "    # Clean ticks\n",
    "    plt.yticks(range(10))\n",
    "    plt.xticks(fontsize=12)\n",
    "    \n",
    "    # Remove spines for cleaner look\n",
    "    sns.despine(left=True, bottom=True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf63de-2528-4e54-96f3-cbe8a461be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plot_chains(low_T,  \"1D Ising Model – Low Temperature (T = 0.5)\\nLarge ferromagnetic domains\")\n",
    "plot_chains(high_T, \"1D Ising Model – High Temperature (T = 3.0)\\nComplete paramagnetic disorder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2d9b4-754a-422b-a90b-3d1c215256a4",
   "metadata": {},
   "source": [
    "### 2. Transfer Matrix Method\n",
    "\n",
    "Exact Analytical Solution of the 1D Ising Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710386f-b3b9-4342-9542-9649e0753ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_spin_correlation_1d(L: int = 32, temp: float = 1.0, J: float = 1.0) -> torch.Tensor:\n",
    "    beta = 1.0 / temp\n",
    "    corr = torch.tanh(torch.tensor(beta * J)) ** torch.arange(L)\n",
    "    corr = torch.roll(corr, -L//2)\n",
    "    return corr.cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(11, 6.5))\n",
    "temps = [0.4, 0.6, 0.8, 1.0, 1.5, 3.0]\n",
    "colors = plt.cm.viridis(torch.linspace(0.9, 0.1, len(temps)))\n",
    "\n",
    "for T in temps:\n",
    "    corr = exact_spin_correlation_1d(temp=T)\n",
    "    plt.plot(range(-16, 16), corr, 'o-', label=rf'$T = {T}$', markersize=7, lw=2.5)\n",
    "\n",
    "plt.axhline(0, color='k', lw=0.8, alpha=0.4)\n",
    "plt.title(r\"Exact Spin-Spin Correlation $\\langle \\sigma_0 \\sigma_r \\rangle$\"\n",
    "          r\" in the 1D Ising Model ($h=0$)\", fontsize=20, pad=25)\n",
    "plt.xlabel(r\"Distance $r$\", fontsize=16)\n",
    "plt.ylabel(r\"Correlation $\\langle \\sigma_0 \\sigma_r \\rangle$\", fontsize=16)\n",
    "plt.legend(fontsize=13, frameon=False)\n",
    "plt.grid(True, alpha=0.3)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd8871-978c-4120-8e7f-ad5c009f491d",
   "metadata": {},
   "source": [
    "### 3. Dataset Preparation – From Spins to Tokens\n",
    "\n",
    "Now we use the data we have generated to prepare the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50261f01-f64d-43e9-8614-b3dd96ef0f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingDataset(Dataset):\n",
    "    def __init__(self, spins: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Convert ±1 spins → {0, 1} tokens for vocabulary size 2.\n",
    "        Each sample becomes an autoregressive sequence: predict next spin from previous ones.\n",
    "        \"\"\"\n",
    "        self.tokens = ((spins + 1) // 2).long()   # ±1 → 0/1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.tokens[idx]\n",
    "        return seq[:-1], seq[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab38eeaf-3ab0-42fc-b457-41db6906444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temperature-mixed dataset\n",
    "data = torch.cat([low_T, mid_T, high_T], dim=0)\n",
    "dataset = IsingDataset(data)\n",
    "loader  = DataLoader(dataset, batch_size=512, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset):,}\")\n",
    "print(f\"Vocabulary size: {dataset.tokens.max().item() + 1}\")\n",
    "print(f\"Example chain (tokens): {dataset[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d3a911-c9ac-44f9-8115-aabce88a383d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.cat([low_T, mid_T, high_T], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74330bca-0182-4f31-8f1a-5e815f65636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Validation\n",
    "temps = [0.5, 1.0, 3.0]\n",
    "for i, T in enumerate(temps):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Validating T = {T}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    samples = data[i*N:(i+1)*N]\n",
    "    results = physai.validate_ising_samples(samples, temp=T, plot=True)\n",
    "    physai.print_validation_report(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ac35c-4ee4-4a2c-aab4-9e56a7852e41",
   "metadata": {},
   "source": [
    "### 3. Build the Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bc296-ad97-45ba-b96e-1ae94d236883",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Single transformer block with causal self-attention\"\"\"\n",
    "    def __init__(self, d_model: int, n_head: int):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            d_model, \n",
    "            n_head, \n",
    "            dropout=0.0, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pre-norm + attention with causal mask\n",
    "        b, t, d = x.shape\n",
    "        # Lower triangular = True (visible), upper = False (masked)\n",
    "        causal_mask = torch.tril(\n",
    "            torch.ones(t, t, device=x.device, dtype=torch.bool), \n",
    "            diagonal=0\n",
    "        )\n",
    "        \n",
    "        x_norm = self.ln1(x)\n",
    "        attn_out, _ = self.attn(\n",
    "            x_norm, x_norm, x_norm,\n",
    "            attn_mask=causal_mask,\n",
    "            need_weights=False\n",
    "        )\n",
    "        x = x + attn_out\n",
    "        \n",
    "        # Pre-norm + MLP\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c850e2-851b-4713-94d4-6a93e88d69f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsingTransformer(nn.Module):\n",
    "    def __init__(self, d_model: int = 128, n_head: int = 8, n_layer: int = 6, block_size: int = 31):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(2, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_head) for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, 2, bias=False)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        # Token + position embeddings\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
    "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        \n",
    "        if targets is None:\n",
    "            return logits\n",
    "        \n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad99e88-923b-46c0-8fa7-f0babb0ba8f3",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a579d93-d963-4883-907f-5924e03b6f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IsingTransformer(d_model=128, n_head=8, n_layer=6).to('mps')\n",
    "\n",
    "history = physai.train_ising_transformer(\n",
    "    model=model,\n",
    "    loader=loader,\n",
    "    num_steps=1_000,\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    device='mps',\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb951a06-54fb-472d-9413-f4bd6891540c",
   "metadata": {},
   "source": [
    "### 5. Check the Model has really learnt it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf4cd8-ddb4-4e9b-a5e9-c0f9d5f14290",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Test 1: Low Temperature\n",
    "    test_ordered = torch.tensor([[0]*20 + [0]*11], device='mps')  # 全 0\n",
    "    logits_ordered = model(test_ordered)\n",
    "    probs_ordered = F.softmax(logits_ordered, dim=-1)\n",
    "    \n",
    "    # Test 2：High Temperature\n",
    "    test_random = torch.tensor([[0, 1, 0, 1, 1, 0, 1, 0, 0, 1] * 3 + [0]], device='mps')\n",
    "    logits_random = model(test_random)\n",
    "    probs_random = F.softmax(logits_random, dim=-1)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"TEST 1: Ordered sequence (all 0s, low-T like)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Last position prediction:\")\n",
    "    print(f\"  P(next=0) = {probs_ordered[0, -1, 0].item():.4f}\")\n",
    "    print(f\"  P(next=1) = {probs_ordered[0, -1, 1].item():.4f}\")\n",
    "    print(f\"Expected: P(0) >> P(1) due to ferromagnetic correlation\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST 2: Random sequence (alternating, high-T like)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Last position prediction:\")\n",
    "    print(f\"  P(next=0) = {probs_random[0, -1, 0].item():.4f}\")\n",
    "    print(f\"  P(next=1) = {probs_random[0, -1, 1].item():.4f}\")\n",
    "    print(f\"Expected: closer to 0.5/0.5 (weak correlation)\")\n",
    "    \n",
    "    # Test 3：Learning nearest-neighbor bias\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEST 3: Nearest-neighbor correlation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Previous 0, predict next\n",
    "    context_0 = torch.tensor([[0]*15], device='mps')\n",
    "    p_next_0 = F.softmax(model(context_0), dim=-1)[0, -1, 0].item()\n",
    "    \n",
    "    # Previous 1, predict next\n",
    "    context_1 = torch.tensor([[1]*15], device='mps')\n",
    "    p_next_1 = F.softmax(model(context_1), dim=-1)[0, -1, 1].item()\n",
    "    \n",
    "    print(f\"After seeing 0000...:\")\n",
    "    print(f\"  P(next=0) = {p_next_0:.4f}\")\n",
    "    print(f\"\\nAfter seeing 1111...:\")\n",
    "    print(f\"  P(next=1) = {p_next_1:.4f}\")\n",
    "    \n",
    "    if p_next_0 > 0.7 and p_next_1 > 0.7:\n",
    "        print(\"\\n✓✓✓ Model learned ferromagnetic correlation!\")\n",
    "        print(\"    (spins prefer to align with neighbors)\")\n",
    "    elif p_next_0 > 0.6 or p_next_1 > 0.6:\n",
    "        print(\"\\n✓ Model learned some correlation structure\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ Model hasn't learned clear correlations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abbe94-27ca-4ff8-bbda-416f835860cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhysAI-Zen (MPS)",
   "language": "python",
   "name": "physai-zen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
