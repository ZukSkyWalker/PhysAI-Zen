<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/PhysAI-Zen/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/PhysAI-Zen/" rel="alternate" type="text/html" /><updated>2025-12-13T22:20:55-08:00</updated><id>http://localhost:4000/PhysAI-Zen/feed.xml</id><title type="html">ZukSkyWalker’s Blog</title><subtitle>Seeking the truth of the universe</subtitle><author><name>Zukai Wang</name></author><entry><title type="html">Chapter 03: Maximum-Entropy RL – Langevin Dynamics + Maximum Caliber</title><link href="http://localhost:4000/PhysAI-Zen/2025/12/10/maxent-rl/" rel="alternate" type="text/html" title="Chapter 03: Maximum-Entropy RL – Langevin Dynamics + Maximum Caliber" /><published>2025-12-10T00:00:00-08:00</published><updated>2025-12-10T00:00:00-08:00</updated><id>http://localhost:4000/PhysAI-Zen/2025/12/10/maxent-rl</id><content type="html" xml:base="http://localhost:4000/PhysAI-Zen/2025/12/10/maxent-rl/"><![CDATA[<blockquote>
  <p>Soft Actor-Critic (SAC) and maximum-entropy reinforcement learning are not ad-hoc tricks — they are the <strong>unique, principled solution</strong> to control under uncertainty, derived from Langevin dynamics and Jaynes’ maximum caliber principle.</p>
</blockquote>

<hr />

<h2 id="physics-background">Physics Background</h2>

<h3 id="langevin-dynamics">Langevin Dynamics</h3>

<p>Consider a particle moving in a potential $V(x)$ at temperature $T$, subject to <strong>overdamped</strong> (high-friction) dynamics:</p>

\[\dot{x} = -\nabla V(x) + \sqrt{2 D} \, \eta(t)\]

<p>where:</p>
<ul>
  <li>$\nabla V(x)$: deterministic force (gradient descent on potential)</li>
  <li>$\eta(t)$: white noise, $\langle \eta(t) \eta(t’) \rangle = \delta(t - t’)$</li>
  <li>$D = k_B T / \gamma$: diffusion coefficient (Einstein relation), where $\gamma$ is friction</li>
</ul>

<p><strong>Key result</strong>: The stationary distribution is the <strong>Boltzmann distribution</strong>:</p>

\[p_{\text{eq}}(x) = \frac{1}{Z} \exp\Big(-\frac{V(x)}{k_B T}\Big)\]

<p>where $Z = \int dx \, \exp(-V(x)/k_B T)$.</p>

<p><strong>Interpretation</strong>: The system <strong>explores the energy landscape</strong> via thermal fluctuations, but spends exponentially more time in low-energy regions. The temperature $T$ controls the exploration-exploitation tradeoff:</p>
<ul>
  <li>High $T$: wide exploration (flat distribution)</li>
  <li>Low $T$: exploitation (concentrate at global minimum)</li>
</ul>

<h3 id="fokker-planck-equation">Fokker-Planck Equation</h3>

<p>The evolution of the probability density $p(x, t)$ under Langevin dynamics is governed by the <strong>Fokker-Planck equation</strong>:</p>

\[\frac{\partial p}{\partial t} = \nabla \cdot \Big[\nabla V(x) \, p + D \, \nabla p\Big]\]

<p>At equilibrium ($\partial p / \partial t = 0$), this yields the Boltzmann distribution.</p>

<p>The figure below shows Langevin dynamics in a double-well potential at different temperatures. The empirical histograms (blue) match the theoretical Boltzmann distribution (red), confirming that temperature controls the exploration-exploitation tradeoff.</p>

<p><img src="../plots/langevin_dynamics_temperature_controls.png" alt="Langevin Dynamics: Temperature Controls Exploration" /></p>

<p><strong>Key observations</strong>:</p>
<ul>
  <li>Low $T$ (0.1): Particle is trapped in one well (exploitation)</li>
  <li>High $T$ (2.0): Particle explores both wells (exploration)</li>
  <li>Empirical distributions converge to theoretical Boltzmann predictions</li>
</ul>

<hr />

<h2 id="maximum-caliber-entropy-over-paths">Maximum Caliber: Entropy Over Paths</h2>

<h3 id="jaynes-maximum-entropy-principle-static">Jaynes’ Maximum Entropy Principle (Static)</h3>

<p>Given constraints $\mathbb{E}_p[f_k(x)] = c_k$, the <strong>least-biased</strong> distribution is:</p>

\[p^*(x) = \arg\max_p H(p) = \arg\max_p \Big(-\int p(x) \log p(x) \, dx\Big)\]

<p>subject to the constraints. The solution is the <strong>exponential family</strong>:</p>

\[p^*(x) = \frac{1}{Z} \exp\Big(-\sum_k \lambda_k f_k(x)\Big)\]

<h3 id="maximum-caliber-dynamic">Maximum Caliber (Dynamic)</h3>

<p>For <strong>trajectories</strong> $x(t)$ over time $[0, T]$, we maximize entropy over <strong>path distributions</strong> $P[x(t)]$:</p>

\[P^*[x(t)] = \arg\max_P \mathcal{H}[P]\]

<p>subject to trajectory-level constraints, e.g.:</p>

\[\mathbb{E}_P\Big[\int_0^T L(x(t), \dot{x}(t)) \, dt\Big] = \bar{L}\]

<p>The solution (Jaynes, 1980) is:</p>

\[P^*[x(t)] \propto \exp\Big(-\int_0^T L(x(t), \dot{x}(t)) \, dt\Big)\]

<p>This is exactly the <strong>path-integral weight</strong> from statistical mechanics!</p>

<p><strong>In RL context</strong>: Replace $L$ with negative reward $-R$, and maximize caliber subject to a constraint on expected cumulative reward. The result is a policy that <strong>maximizes both reward and entropy</strong>.</p>

<hr />

<h2 id="maximum-entropy-reinforcement-learning">Maximum-Entropy Reinforcement Learning</h2>

<h3 id="standard-rl-objective">Standard RL Objective</h3>

<table>
  <tbody>
    <tr>
      <td>Find a policy $\pi(a</td>
      <td>s)$ that maximizes expected return:</td>
    </tr>
  </tbody>
</table>

\[J(\pi) = \mathbb{E}_{\tau \sim \pi} \Big[\sum_{t=0}^{T} \gamma^t r_t\Big]\]

<p>where $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots)$ is a trajectory sampled by following $\pi$.</p>

<p><strong>Problem</strong>: This often leads to:</p>
<ul>
  <li><strong>Brittle policies</strong>: deterministic, overfit to specific environment dynamics</li>
  <li><strong>Poor exploration</strong>: gets stuck in local optima</li>
  <li><strong>Lack of robustness</strong>: fails under perturbations</li>
</ul>

<h3 id="maxent-rl-objective">MaxEnt RL Objective</h3>

<p>Add an <strong>entropy regularization</strong> term:</p>

\[J_{\text{MaxEnt}}(\pi) = \mathbb{E}_{\tau \sim \pi} \Big[\sum_{t=0}^{T} \gamma^t \big(r_t + \alpha H(\pi(\cdot | s_t))\big)\Big]\]

<p>where:</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$H(\pi(\cdot</td>
          <td>s)) = -\sum_a \pi(a</td>
          <td>s) \log \pi(a</td>
          <td>s)$: entropy of the policy at state $s$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>$\alpha &gt; 0$: temperature parameter (controls exploration)</li>
</ul>

<p><strong>Intuition</strong>: Among all policies achieving the same expected reward, prefer the <strong>most random</strong> one (maximum entropy). This ensures:</p>
<ul>
  <li><strong>Exploration</strong>: the policy doesn’t collapse to a single action</li>
  <li><strong>Robustness</strong>: spread probability mass over multiple good actions</li>
  <li><strong>Transfer learning</strong>: learned skills are more general</li>
</ul>

<h3 id="connection-to-langevin-dynamics">Connection to Langevin Dynamics</h3>

<p>Rewrite the value function for MaxEnt RL:</p>

\[V^\pi(s) = \mathbb{E}_{\pi} \Big[\sum_{t=0}^{\infty} \gamma^t (r_t + \alpha H(\pi(\cdot | s_t))) \Big| s_0 = s\Big]\]

<p>Define a “free energy” (Helmholtz analogy):</p>

\[F^\pi(s) = -V^\pi(s) = -\mathbb{E}[R] - \alpha H(\pi)\]

<p>The optimal policy minimizes $F^\pi(s)$ — this is exactly the <strong>variational principle</strong> in statistical mechanics.</p>

<p>In continuous time, the policy update can be written as a <strong>gradient flow</strong>:</p>

\[\frac{\partial \pi}{\partial t} \propto -\nabla_\pi F(\pi) = \nabla_\pi \Big(\mathbb{E}_\pi[R] + \alpha H(\pi)\Big)\]

<p>This is <strong>Langevin dynamics in policy space</strong>, where:</p>
<ul>
  <li>“Energy” $\leftrightarrow$ negative reward $-R$</li>
  <li>“Temperature” $\leftrightarrow$ entropy coefficient $\alpha$</li>
</ul>

<hr />

<h2 id="algorithm-soft-actor-critic-sac">Algorithm: Soft Actor-Critic (SAC)</h2>

<p>SAC (Haarnoja et al., 2018) is the <strong>state-of-the-art</strong> continuous-control algorithm based on MaxEnt RL.</p>

<h3 id="soft-bellman-equation">Soft Bellman Equation</h3>

<p>Define the <strong>soft Q-function</strong>:</p>

\[Q^*(s, a) = r(s, a) + \gamma \, \mathbb{E}_{s' \sim p(\cdot|s,a)} \Big[V^*(s')\Big]\]

<p>where the <strong>soft value function</strong> is:</p>

\[V^*(s) = \mathbb{E}_{a \sim \pi^*(\cdot|s)} \Big[Q^*(s, a) - \alpha \log \pi^*(a|s)\Big]\]

<p>Equivalently:</p>

\[V^*(s) = \alpha \log \int \exp\Big(\frac{1}{\alpha} Q^*(s, a)\Big) da\]

<p>This is a <strong>soft maximum</strong> (LogSumExp). As $\alpha \to 0$, it reduces to the standard $V^<em>(s) = \max_a Q^</em>(s, a)$.</p>

<h3 id="optimal-policy">Optimal Policy</h3>

<p>The optimal policy in MaxEnt RL is:</p>

\[\pi^*(a|s) = \frac{1}{Z(s)} \exp\Big(\frac{1}{\alpha} Q^*(s, a)\Big)\]

<p>where $Z(s) = \int \exp(Q^*(s,a)/\alpha) da$ is the partition function.</p>

<p><strong>This is exactly the Boltzmann distribution</strong> over actions, with:</p>
<ul>
  <li>“Energy” $= -Q^*(s, a)$</li>
  <li>“Temperature” $= \alpha$</li>
</ul>

<p>The figure below demonstrates how the temperature parameter $\alpha$ controls the policy distribution over actions with different Q-values:</p>

<p><img src="../plots/Boltzmann_Policy.png" alt="Boltzmann Policy: Temperature Controls Exploration-Exploitation" /></p>

<p><strong>Key observations</strong>:</p>
<ul>
  <li>$\alpha \to 0$: Policy becomes deterministic (selects highest Q-value action, shown in red)</li>
  <li>$\alpha \to \infty$: Policy becomes uniform (maximum exploration)</li>
  <li>Intermediate $\alpha$: Balances exploitation (high Q-values) with exploration (entropy)</li>
  <li>Higher $\alpha$ leads to higher entropy $H(\pi)$ and more distributed probability mass</li>
</ul>

<h3 id="sac-algorithm">SAC Algorithm</h3>

<p>SAC maintains:</p>
<ol>
  <li><strong>Soft Q-networks</strong>: $Q_\phi(s, a)$ (two networks for stability, take minimum)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Policy network</strong>: $\pi_\theta(a</td>
          <td>s)$ (Gaussian for continuous actions)</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Target networks</strong>: $Q_{\phi’}$ (slowly updated via Polyak averaging)</li>
</ol>

<p><strong>Training loop</strong>:</p>

<p><strong>1. Critic update</strong> (minimize soft Bellman error):</p>

\[L_Q(\phi) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \Big[\big(Q_\phi(s,a) - y\big)^2\Big]\]

<p>where the target is:</p>

\[y = r + \gamma \Big(\min_{i=1,2} Q_{\phi_i'}(s', a') - \alpha \log \pi_\theta(a'|s')\Big), \quad a' \sim \pi_\theta(\cdot | s')\]

<p><strong>2. Actor update</strong> (maximize expected soft Q-value):</p>

\[L_\pi(\theta) = \mathbb{E}_{s \sim \mathcal{D}, a \sim \pi_\theta(\cdot|s)} \Big[\alpha \log \pi_\theta(a|s) - Q_\phi(s, a)\Big]\]

<p>This is equivalent to minimizing the KL divergence:</p>

\[\text{KL}\Big(\pi_\theta(\cdot|s) \,\Big\|\, \frac{1}{Z(s)} \exp\big(Q_\phi(s, \cdot)/\alpha\big)\Big)\]

<p><strong>3. Temperature auto-tuning</strong> (optional):</p>

<p>Adjust $\alpha$ to maintain a target entropy $\bar{H}$:</p>

\[L_\alpha = -\mathbb{E}_{a \sim \pi_\theta} \Big[\alpha \big(\log \pi_\theta(a|s) + \bar{H}\big)\Big]\]

<hr />

<h2 id="ml-narrative-why-maxent-wins">ML Narrative: Why MaxEnt Wins</h2>

<h3 id="comparison-with-standard-rl">Comparison with Standard RL</h3>

<table>
  <thead>
    <tr>
      <th>Standard RL</th>
      <th>MaxEnt RL (SAC)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Policy: deterministic or $\epsilon$-greedy</td>
      <td>Policy: stochastic Boltzmann</td>
    </tr>
    <tr>
      <td>Exploration: external (e.g., noise injection)</td>
      <td>Exploration: intrinsic (entropy bonus)</td>
    </tr>
    <tr>
      <td>Bellman equation: $Q(s,a) = r + \gamma \max_{a’} Q(s’,a’)$</td>
      <td>Soft Bellman: $Q(s,a) = r + \gamma \, \mathbb{E}[\text{softmax}_\alpha Q(s’,\cdot)]$</td>
    </tr>
    <tr>
      <td>Overestimates value (max bias)</td>
      <td>Less biased (soft max)</td>
    </tr>
    <tr>
      <td>Brittle to environment changes</td>
      <td>Robust (multi-modal policy)</td>
    </tr>
  </tbody>
</table>

<h3 id="empirical-advantages">Empirical Advantages</h3>

<ol>
  <li><strong>Sample efficiency</strong>: SAC matches or beats PPO/DDPG on most continuous-control benchmarks</li>
  <li><strong>Stability</strong>: Entropy regularization prevents policy collapse, smooths optimization</li>
  <li><strong>Off-policy</strong>: Can reuse old data (unlike on-policy methods like PPO)</li>
  <li><strong>Automatic exploration</strong>: No need to manually tune exploration noise</li>
</ol>

<h3 id="training-results-on-pendulum-v1">Training Results on Pendulum-v1</h3>

<p>The figure below shows SAC training on the continuous control task Pendulum-v1 (swing-up and balance):</p>

<p><img src="../plots/RL_training_log.png" alt="SAC Training on Pendulum-v1" /></p>

<p><strong>Key observations</strong>:</p>
<ul>
  <li><strong>Learning Curve</strong> (left): Episode reward improves from ~-1500 (random) to ~-300, showing learning progress. The smoothed curve (dark blue) shows clear improvement despite high variance.</li>
  <li><strong>Exploration</strong> (middle): Policy entropy decreases over time as the agent becomes more confident, but remains non-zero (maintains stochasticity). This is a key advantage of MaxEnt RL.</li>
  <li><strong>Training Losses</strong> (right): Both Q-loss (green) and policy loss (purple) stabilize after initial growth, indicating convergence of the learning process.</li>
</ul>

<p><strong>Note</strong>: Pendulum uses a cost-based reward (negative values), where 0 is optimal (upright and stationary). Well-trained SAC typically achieves -150 to -300 on this task.</p>

<h3 id="learned-policy-and-value-function">Learned Policy and Value Function</h3>

<p>The figure below visualizes what the trained SAC agent has learned by examining the policy actions and Q-values across the state space:</p>

<p><img src="../plots/policy_and_Q.png" alt="Policy Actions and Q-Values in State Space" /></p>

<p><strong>Key observations</strong>:</p>
<ul>
  <li><strong>Policy Actions</strong> (left): The learned policy $\pi(s)$ applies smooth, continuous torque control across different angles and angular velocities. The policy exhibits intelligent structure:
    <ul>
      <li>Near upright position ($\theta \approx 0$): Applies corrective torque based on angular velocity to maintain balance</li>
      <li>Far from upright: Applies consistent torque to swing the pendulum toward the goal</li>
      <li>Color gradient (red to blue) shows the direction and magnitude of applied torque</li>
    </ul>
  </li>
  <li>
    <p><strong>Q-Values</strong> (right): The soft Q-function $Q(s, \pi(s))$ shows highest values (yellow) near the upright position with low velocity, correctly identifying this as the most desirable state. The value landscape smoothly decreases as the pendulum moves away from the goal.</p>
  </li>
  <li><strong>Smooth Control Strategy</strong>: Unlike discrete or bang-bang controllers, the MaxEnt policy learns a smooth, differentiable control law that generalizes well across the continuous state space.</li>
</ul>

<p>This visualization confirms that SAC successfully learned the underlying physics of the pendulum and developed a sophisticated control strategy that balances reaching the goal (high Q-values) with maintaining exploration (stochastic policy).</p>

<h3 id="connection-to-physics">Connection to Physics</h3>

<p>The <strong>Boltzmann policy</strong></p>

\[\pi^*(a|s) \propto \exp\Big(\frac{Q^*(s,a)}{\alpha}\Big)\]

<p>is not a heuristic — it’s the <strong>unique solution</strong> to:</p>

\[\max_\pi \, \mathbb{E}_\pi[Q(s, \cdot)] + \alpha H(\pi)\]

<p>This is the same variational problem solved by the <strong>canonical ensemble</strong> in statistical mechanics:</p>

\[\max_p \, \mathbb{E}_p[E] - \frac{1}{\beta} H(p) \quad \Rightarrow \quad p^*(x) \propto e^{-\beta E(x)}\]

<p>In other words: <strong>SAC is not “inspired by” physics; it <em>is</em> physics applied to control</strong>.</p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>MaxEnt RL = Langevin Dynamics</strong>: The policy update in SAC is mathematically equivalent to running Langevin dynamics in policy space, where reward is “negative energy” and $\alpha$ is temperature.</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Boltzmann Policy is Optimal</strong>: The exponential form $\pi(a</td>
          <td>s) \propto \exp(Q(s,a)/\alpha)$ is not arbitrary — it’s the unique solution to maximizing expected return under an entropy constraint.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p><strong>Physics Provides the Principle</strong>: Just as the canonical ensemble in stat mech is derived from maximum entropy, MaxEnt RL is derived from maximum caliber over trajectories.</p>
  </li>
  <li>
    <p><strong>Temperature Controls Exploration</strong>: The parameter $\alpha$ is not a “hyperparameter” — it’s the thermodynamic temperature of the policy. High $\alpha$ = more exploration; low $\alpha$ = more exploitation.</p>
  </li>
  <li>
    <p><strong>Soft Bellman = Smooth Value</strong>: The soft Bellman equation $V(s) = \alpha \log \int e^{Q(s,a)/\alpha} da$ is a temperature-smoothed version of $V(s) = \max_a Q(s,a)$, reducing overestimation bias.</p>
  </li>
  <li><strong>Unified View</strong>: From Ising spins (Chapter 01) to trajectories (Chapter 02) to policies (Chapter 03), the <strong>Boltzmann distribution</strong> is the universal language connecting statistical physics and machine learning.</li>
</ol>

<hr />

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><strong>Original SAC paper</strong>: Haarnoja et al., <em>Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL</em> (ICML 2018)</li>
  <li><strong>MaxEnt RL framework</strong>: Levine, <em>Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review</em> (arXiv:1805.00909)</li>
  <li><strong>Maximum Caliber</strong>: Jaynes, <em>The Minimum Entropy Production Principle</em> (1980); Pressé et al., <em>Principles of maximum entropy and maximum caliber in statistical physics</em> (RMP 2013)</li>
  <li><strong>Langevin RL</strong>: Nachum et al., <em>Bridging the Gap Between Value and Policy Based RL</em> (NeurIPS 2017)</li>
  <li><strong>Thermodynamics of RL</strong>: Ortega &amp; Braun, <em>Thermodynamics as a theory of decision-making with information-processing costs</em> (2013)</li>
</ul>]]></content><author><name>ZukSkyWalker</name></author><category term="Maximum Entropy" /><category term="Soft Actor-Critic" /><category term="Reinforcement Learning" /><category term="Langevin Dynamics" /><category term="Statistical Physics" /><summary type="html"><![CDATA[Soft Actor-Critic (SAC) and maximum-entropy reinforcement learning are not ad-hoc tricks — they are the unique, principled solution to control under uncertainty, derived from Langevin dynamics and Jaynes' maximum caliber principle.]]></summary></entry><entry><title type="html">Chapter 02: Decision Transformer – Offline RL as Discrete Path Integral</title><link href="http://localhost:4000/PhysAI-Zen/2025/12/08/decision-transformer/" rel="alternate" type="text/html" title="Chapter 02: Decision Transformer – Offline RL as Discrete Path Integral" /><published>2025-12-08T00:00:00-08:00</published><updated>2025-12-08T00:00:00-08:00</updated><id>http://localhost:4000/PhysAI-Zen/2025/12/08/decision-transformer</id><content type="html" xml:base="http://localhost:4000/PhysAI-Zen/2025/12/08/decision-transformer/"><![CDATA[<blockquote>
  <p>Offline reinforcement learning recast as sequence modeling: trajectories are “paths” in state-action space, and the Transformer learns a distribution over trajectories weighted by cumulative reward — exactly analogous to a path integral with action $S = -\sum R_t$.</p>
</blockquote>

<hr />

<h2 id="physics-background">Physics Background</h2>

<h3 id="path-integrals-in-quantum-mechanics">Path Integrals in Quantum Mechanics</h3>

<p>In Feynman’s formulation, the transition amplitude from state $x_0$ at $t=0$ to $x_T$ at time $T$ is:</p>

\[\langle x_T | e^{-iHT/\hbar} | x_0 \rangle = \int \mathcal{D}x(t) \, \exp\Big(\frac{i}{\hbar} S[x(t)]\Big)\]

<p>where the <strong>action</strong> is:</p>

\[S[x(t)] = \int_0^T dt \, \Big[\frac{m}{2}\dot{x}^2 - V(x)\Big]\]

<p>In the <strong>Euclidean (imaginary-time)</strong> version ($t \to -i\tau$), this becomes:</p>

\[Z = \int \mathcal{D}x(\tau) \, \exp\big(-S_E[x(\tau)]\big)\]

<p>with $S_E = \int d\tau \, [\frac{m}{2}\dot{x}^2 + V(x)]$. Now it looks like <strong>statistical mechanics</strong>: paths with lower action have exponentially higher weight.</p>

<h3 id="discrete-path-integrals">Discrete Path Integrals</h3>

<p>For a system evolving in discrete time steps $t = 0, 1, \dots, T$:</p>

\[Z = \sum_{\text{all paths } x_0, x_1, \dots, x_T} \exp\big(-S_{\text{discrete}}[x_0, \dots, x_T]\big)\]

<p>where the discrete action might be:</p>

\[S_{\text{discrete}} = \sum_{t=0}^{T-1} \Big[\frac{(x_{t+1} - x_t)^2}{2\Delta t} + V(x_t) \Delta t\Big]\]

<p><strong>Key idea</strong>: The system “explores all possible paths” but exponentially favors paths with low action.</p>

<h3 id="mapping-physics-to-rl">Mapping Physics to RL</h3>

<p>In the discrete action above, we can identify two components:</p>

<ol>
  <li><strong>Kinetic term</strong> (dynamics): $\frac{(x_{t+1} - x_t)^2}{2\Delta t}$ penalizes large jumps between consecutive states</li>
  <li><strong>Potential term</strong> (cost): $V(x_t) \Delta t$ assigns a cost to visiting state $x_t$</li>
</ol>

<p>In <strong>reinforcement learning</strong>, the analogous structure is:</p>

\[S_{\text{RL}}[\tau] = \sum_{t=0}^{T-1} \Big[\underbrace{-\log p(s_{t+1} | s_t, a_t)}_{\text{dynamics constraint}} + \underbrace{(-r_t)}_{\text{negative reward}}\Big]\]

<p><strong>Correspondence</strong>:</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Dynamics</strong> $p(s_{t+1}</td>
          <td>s_t, a_t)$ ↔ <strong>Constrained paths</strong>: The MDP transition probabilities act like a “kinetic term” that constrains which state transitions are likely. Deterministic dynamics ($s_{t+1} = f(s_t, a_t)$) correspond to zero kinetic energy—the path is fully constrained.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Reward</strong> $r_t = R(s_t, a_t)$ ↔ <strong>Negative potential</strong>: Reward plays the role of negative potential energy $-V(x_t)$. High reward = low action, making the trajectory more probable.</li>
</ul>

<p>This gives the <strong>trajectory distribution</strong>:</p>

\[p(\tau) = p(s_0) \prod_{t=0}^{T-1} p(s_{t+1} | s_t, a_t) \cdot \pi(a_t | s_t) \cdot \exp\Big(\frac{1}{\alpha} r_t\Big)\]

<table>
  <tbody>
    <tr>
      <td>When we marginalize over the dynamics (which are fixed by the environment), we recover the MaxEnt RL objective where the policy $\pi(a</td>
      <td>s)$ implicitly learns to sample high-reward trajectories.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="rl-as-path-integral-over-trajectories">RL as Path Integral Over Trajectories</h2>

<h3 id="reinforcement-learning-setup">Reinforcement Learning Setup</h3>

<p>An agent interacts with an environment over discrete time steps:</p>
<ul>
  <li><strong>State</strong>: $s_t \in \mathcal{S}$</li>
  <li><strong>Action</strong>: $a_t \in \mathcal{A}$</li>
  <li><strong>Reward</strong>: $r_t = R(s_t, a_t)$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Dynamics</strong>: $s_{t+1} \sim p(s’</td>
          <td>s_t, a_t)$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>A <strong>trajectory</strong> is a sequence:</p>

\[\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_T, a_T, r_T)\]

<p>The <strong>return</strong> (cumulative reward) is:</p>

\[G(\tau) = \sum_{t=0}^{T} \gamma^t r_t\]

<p>where $\gamma \in [0,1]$ is the discount factor.</p>

<h3 id="maximum-entropy-rl-brief-recap-from-chapter-03">Maximum Entropy RL (brief recap from Chapter 03)</h3>

<table>
  <tbody>
    <tr>
      <td>In MaxEnt RL, we seek a policy $\pi(a</td>
      <td>s)$ that maximizes:</td>
    </tr>
  </tbody>
</table>

\[J(\pi) = \mathbb{E}_\tau \Big[\sum_t r_t\Big] + \alpha H(\pi)\]

<p>The optimal policy has the form:</p>

\[\pi^*(a|s) \propto \exp\Big(\frac{1}{\alpha} Q^*(s, a)\Big)\]

<p>This is a <strong>Boltzmann policy</strong> with “temperature” $\alpha$.</p>

<h3 id="trajectory-distribution-as-path-integral">Trajectory Distribution as Path Integral</h3>

<p>We can define a <strong>distribution over trajectories</strong> weighted by return:</p>

\[p(\tau) \propto \exp\Big(\frac{1}{\alpha} G(\tau)\Big)\]

<p>Rewrite $G(\tau) = \sum_t \gamma^t r_t$ as a “negative action”:</p>

\[S[\tau] := -\sum_t \gamma^t r_t\]

<p>Then:</p>

\[p(\tau) = \frac{1}{Z} \exp\big(-S[\tau] / \alpha\big)\]

<p>where $Z = \sum_{\tau} \exp(-S[\tau]/\alpha)$ is the trajectory partition function.</p>

<p><strong>This is exactly the discrete path integral formulation</strong>: trajectories with higher reward (lower “action” $S$) have exponentially higher probability.</p>

<hr />

<h2 id="algorithm-decision-transformer">Algorithm: Decision Transformer</h2>

<p><strong>Key insight</strong> (Chen et al., NeurIPS 2021):<br />
Instead of learning a value function $Q(s,a)$ or policy $\pi(a|s)$, directly model the <strong>conditional distribution</strong>:</p>

\[p(a_t \mid s_0, a_0, r_0, \dots, s_t, \hat{R}_t)\]

<p>where $\hat{R}_t$ is the <strong>desired return-to-go</strong> at time $t$.</p>

<p>At test time:</p>
<ol>
  <li>Specify a high target return $\hat{R}_0$ (e.g., the maximum return seen in the dataset)</li>
  <li>Sample actions autoregressively:
\(a_t \sim p_\theta(a_t \mid s_{0:t}, a_{0:t-1}, r_{0:t-1}, \hat{R}_t)\)</li>
  <li>Update return-to-go: $\hat{R}_{t+1} = \hat{R}_t - r_t$</li>
</ol>

<p><strong>Why this works</strong>: The model learns that “if I’m in state $s$ and want to achieve return $\hat{R}$, I should take action $a$ that historically led to $\hat{R}$ from $s$.”</p>

<h3 id="architecture">Architecture</h3>

<p>Treat the trajectory as a sequence of <strong>interleaved tokens</strong>:</p>

\[(\hat{R}_0, s_0, a_0, \hat{R}_1, s_1, a_1, \dots, \hat{R}_T, s_T, a_T)\]

<p>Use a <strong>causal Transformer</strong> with:</p>
<ul>
  <li><strong>Token embeddings</strong>:
    <ul>
      <li>$\hat{R}_t$: scalar → linear layer → $\mathbb{R}^{d}$</li>
      <li>$s_t$: vector or image → embedding layer → $\mathbb{R}^{d}$</li>
      <li>$a_t$: discrete or continuous → embedding layer → $\mathbb{R}^{d}$</li>
    </ul>
  </li>
  <li><strong>Positional embeddings</strong>: learned embeddings for timestep $t$</li>
  <li><strong>Causal attention</strong>: tokens at time $t$ can only attend to $t’ \leq t$</li>
</ul>

<p>Output: predict $a_t$ given all tokens up to $(s_t, \hat{R}_t)$.</p>

<p><strong>Training objective</strong>: supervised learning on offline trajectories</p>

\[\max_\theta \sum_{t=0}^{T} \log p_\theta(a_t \mid \hat{R}_{0:t}, s_{0:t}, a_{0:t-1})\]

<p>where $\hat{R}<em>t = \sum</em>{t’=t}^{T} \gamma^{t’-t} r_{t’}$ is computed from the recorded trajectory.</p>

<hr />

<h2 id="ml-narrative-trajectory-modeling-vs-dynamic-programming">ML Narrative: Trajectory Modeling vs. Dynamic Programming</h2>

<h3 id="traditional-rl-value-functions">Traditional RL: Value Functions</h3>

<p>Most RL algorithms (Q-learning, Actor-Critic, PPO) learn:</p>
<ul>
  <li><strong>Value function</strong>: $V^\pi(s) = \mathbb{E}<em>\pi[\sum</em>{t} \gamma^t r_t \mid s_0 = s]$</li>
  <li><strong>Q-function</strong>: $Q^\pi(s, a) = \mathbb{E}<em>\pi[\sum</em>{t} \gamma^t r_t \mid s_0 = s, a_0 = a]$</li>
</ul>

<p>These satisfy the <strong>Bellman equation</strong> (dynamic programming):</p>

\[Q^\pi(s, a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot|s,a)} [V^\pi(s')]\]

<p>This is a <strong>local, recursive</strong> update: the value at $s$ depends only on the immediate reward and the value of the next state.</p>

<h3 id="decision-transformer-sequence-modeling">Decision Transformer: Sequence Modeling</h3>

<p>Instead, Decision Transformer:</p>
<ol>
  <li>Treats the <strong>entire trajectory</strong> as a sequence</li>
  <li>Uses <strong>global attention</strong> over all timesteps</li>
  <li>Learns the conditional distribution $p(a_t \mid \text{history}, \hat{R}_t)$</li>
</ol>

<p><strong>Advantages</strong>:</p>
<ul>
  <li>No bootstrapping errors (no Bellman backup)</li>
  <li>No off-policy correction needed (just supervised learning)</li>
  <li>Naturally handles long-horizon dependencies (via attention)</li>
  <li>Can be trained on <strong>offline data</strong> without ever interacting with the environment</li>
</ul>

<p><strong>Tradeoffs</strong>:</p>
<ul>
  <li>Requires large, diverse offline datasets</li>
  <li>Doesn’t improve beyond the best trajectory in the dataset</li>
  <li>Less sample-efficient than model-based RL (but more stable)</li>
</ul>

<hr />

<h2 id="connection-to-path-integrals">Connection to Path Integrals</h2>

<h3 id="why-path-integral">Why “Path Integral”?</h3>

<p>In the trajectory distribution</p>

\[p(\tau) \propto \exp\Big(\frac{1}{\alpha} \sum_t \gamma^t r_t\Big),\]

<p>the sum $\sum_t \gamma^t r_t$ plays the role of a <strong>discrete action</strong>. The Decision Transformer implicitly learns this distribution by:</p>
<ol>
  <li>Conditioning on return-to-go $\hat{R}_t$ (≈ “boundary condition” in path integrals)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Modeling $p(a_t</td>
          <td>s_t, \hat{R}_t, \text{past})$ (≈ “path propagator”)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p>At inference, by setting a high $\hat{R}_0$, we <strong>bias the sampling toward high-reward paths</strong>, just as in a path integral, we can bias toward paths with low action by adjusting boundary conditions or sources.</p>

<h3 id="analogy-table">Analogy Table</h3>

<table>
  <thead>
    <tr>
      <th>Path Integral (Physics)</th>
      <th>Decision Transformer (RL)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Path $x(t)$</td>
      <td>Trajectory $\tau = (s_t, a_t, r_t)$</td>
    </tr>
    <tr>
      <td>Action $S[x(t)] = \int L(x, \dot{x}) dt$</td>
      <td>Negative return $S[\tau] = -\sum_t \gamma^t r_t$</td>
    </tr>
    <tr>
      <td>Kinetic term $\frac{m}{2}\dot{x}^2$</td>
      <td>Dynamics constraint $-\log p(s_{t+1} \mid s_t, a_t)$</td>
    </tr>
    <tr>
      <td>Potential $V(x)$</td>
      <td>Negative reward $-r_t$</td>
    </tr>
    <tr>
      <td>Weight $\exp(-S[x]/\hbar)$</td>
      <td>Probability $\exp(\sum_t r_t / \alpha)$</td>
    </tr>
    <tr>
      <td>Partition function $Z = \int \mathcal{D}x \, e^{-S[x]}$</td>
      <td>Trajectory partition function $Z = \sum_\tau e^{G(\tau)/\alpha}$</td>
    </tr>
    <tr>
      <td>Boundary condition $x(0), x(T)$</td>
      <td>Desired return $\hat{R}_0$, initial state $s_0$</td>
    </tr>
    <tr>
      <td>Propagator $K(x_T, x_0)$</td>
      <td>Trajectory distribution $p(\tau \mid s_0, \hat{R}_0)$</td>
    </tr>
  </tbody>
</table>

<p><strong>Key insight</strong>: The Decision Transformer is a <strong>discrete, learned approximation</strong> to the trajectory propagator. By conditioning on return-to-go $\hat{R}_t$, it learns which actions lead to high-reward paths, analogous to how a path integral weights paths by their action.</p>

<table>
  <tbody>
    <tr>
      <td>Note that in the RL setting, the dynamics $p(s_{t+1}</td>
      <td>s_t, a_t)$ are given by the environment and constrain which trajectories are possible (like a “kinetic term” restricting path smoothness), while the policy chooses actions to maximize reward (minimize “potential energy” $-r_t$).</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>RL = Sequence Modeling</strong>: By treating trajectories as sequences and conditioning on return-to-go, we bypass the need for value functions and Bellman equations.</p>
  </li>
  <li>
    <p><strong>Path Integral View</strong>: The trajectory distribution $p(\tau) \propto \exp(G(\tau)/\alpha)$ is mathematically identical to a discrete path integral with action $S = -G(\tau)$.</p>
  </li>
  <li>
    <p><strong>Offline RL Made Simple</strong>: Decision Transformer reduces offline RL to supervised learning — no off-policy corrections, no bootstrapping, just maximum likelihood.</p>
  </li>
  <li>
    <p><strong>Attention = Long-Range Dependencies</strong>: Unlike Markovian methods (Q-learning), the Transformer can discover and exploit correlations across long time horizons.</p>
  </li>
  <li>
    <p><strong>Physics → ML</strong>: The path integral formulation, developed for quantum mechanics, provides a unifying language for understanding both equilibrium statistical mechanics (Chapter 01: Ising) and sequential decision-making (this chapter).</p>
  </li>
</ol>

<hr />

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><strong>Original paper</strong>: Chen et al., <em>Decision Transformer: Reinforcement Learning via Sequence Modeling</em> (NeurIPS 2021)</li>
  <li><strong>Path integrals in RL</strong>: Kappen, <em>Path integrals and symmetry breaking for optimal control theory</em> (2005)</li>
  <li><strong>MaxEnt RL</strong>: Levine, <em>Reinforcement Learning and Control as Probabilistic Inference</em> (arXiv:1805.00909)</li>
  <li><strong>Physics connection</strong>: Feynman &amp; Hibbs, <em>Quantum Mechanics and Path Integrals</em></li>
</ul>]]></content><author><name>ZukSkyWalker</name></author><category term="Decision Transformer" /><category term="Reinforcement Learning" /><category term="Path Integral" /><category term="Offline RL" /><summary type="html"><![CDATA[Offline reinforcement learning recast as sequence modeling: trajectories are 'paths' in state-action space, and the Transformer learns a distribution over trajectories weighted by cumulative reward — exactly analogous to a path integral with action S = -∑Rₜ.]]></summary></entry><entry><title type="html">Chapter 01: IsingGPT – Transformer Learns Phase Transitions</title><link href="http://localhost:4000/PhysAI-Zen/2025/12/01/ising-gpt/" rel="alternate" type="text/html" title="Chapter 01: IsingGPT – Transformer Learns Phase Transitions" /><published>2025-12-01T00:00:00-08:00</published><updated>2025-12-01T00:00:00-08:00</updated><id>http://localhost:4000/PhysAI-Zen/2025/12/01/ising-gpt</id><content type="html" xml:base="http://localhost:4000/PhysAI-Zen/2025/12/01/ising-gpt/"><![CDATA[<blockquote>
  <p>A minimal 2-layer Transformer trained on equilibrium samples from the 1D Ising model spontaneously discovers the Boltzmann distribution, nearest-neighbor spin correlations, and phase-transition behavior — without ever seeing the Hamiltonian. Kudos to Andrej Karpathy’s, this chapter is inspired by his nanoGPT (https://github.com/karpathy/nanoGPT).</p>
</blockquote>

<hr />

<h2 id="physics-background">Physics Background</h2>

<h3 id="the-1d-ising-model">The 1D Ising Model</h3>

<p>The Ising model is the “hydrogen atom” of statistical mechanics: simple enough to solve analytically, rich enough to exhibit nontrivial collective behavior.</p>

<p><strong>Hamiltonian</strong>:</p>

\[H[\{s_i\}] = -J \sum_{i=1}^{L} s_i s_{i+1} - h \sum_{i=1}^{L} s_i\]

<p>where:</p>
<ul>
  <li>$s_i \in {-1, +1}$: spin at site $i$ (periodic boundary: $s_{L+1} = s_1$)</li>
  <li>$J &gt; 0$: ferromagnetic coupling (prefers alignment $\uparrow\uparrow$)</li>
  <li>$h$: external magnetic field</li>
  <li>$L$: chain length</li>
</ul>

<p><strong>Boltzmann distribution</strong> at temperature $T$:</p>

\[p_T(\{s_i\}) = \frac{1}{Z(T)} \exp\Big(-\frac{H[\{s_i\}]}{k_B T}\Big)\]

<p>where $Z(T) = \sum_{{s_i}} \exp(-H/k_B T)$ is the partition function.</p>

<h3 id="phase-transition-in-infinite-1d-none-but-crossover-exists">Phase Transition (in infinite 1D: none; but crossover exists)</h3>

<p>In 1D with finite $L$, there is no true phase transition, but:</p>
<ul>
  <li><strong>High $T$ (paramagnetic)</strong>: spins are disordered, $\langle s_i s_j \rangle \to 0$ rapidly</li>
  <li><strong>Low $T$ (ordered)</strong>: spins align, $\langle s_i s_j \rangle \approx 1$ for all $i, j$</li>
</ul>

<p><strong>Key observable: two-point correlation function</strong></p>

\[C(r) = \langle s_i s_{i+r} \rangle - \langle s_i \rangle^2\]

<p>At $h=0$ (zero field), this decays as</p>

\[C(r) \sim e^{-r/\xi(T)}\]

<p>where $\xi(T) = -1/\log(\tanh(J/k_B T))$ is the <strong>correlation length</strong>. As $T \to 0$, $\xi \to \infty$ (quasi-long-range order).</p>

<p>To illustrate the phase diagram, we generate samples across a grid of temperatures $T$ and fields $h$, verifying that observables like magnetization transition smoothly between ordered (low $T$, small $h$) and disordered (high $T$) regimes:</p>

<p><img src="../plots/data_valid_ht.png" alt="Validation grid over field and temperature" /></p>

<hr />

<h2 id="algorithm-metropolis-hastings-sampling">Algorithm: Metropolis-Hastings Sampling</h2>

<p>We generate equilibrium samples from $p_T({s_i})$ using the <strong>Metropolis algorithm</strong>:</p>

<ol>
  <li>Initialize spins randomly: $s_i \in {-1, +1}$</li>
  <li><strong>Sweep</strong> (repeat $L$ times):
    <ul>
      <li>Pick a random site $i$</li>
      <li>Compute energy change if we flip $s_i \to -s_i$:
\(\Delta E = 2 J s_i (s_{i-1} + s_{i+1}) + 2 h s_i\)</li>
      <li>Accept flip with probability:
\(\min\Big(1, \exp\big(-\Delta E / k_B T\big)\Big)\)</li>
    </ul>
  </li>
  <li>After equilibration (500–1000 sweeps), save configuration</li>
  <li>Wait 10–20 sweeps between samples (decorrelation)</li>
</ol>

<p><strong>Implementation sketch</strong> (<code class="language-plaintext highlighter-rouge">src/ising.py</code>), as a flowchart:</p>

<pre><code class="language-mermaid">flowchart TD
    A[Start: random spins sᵢ taking values -1 or +1] --&gt; B[Set β = 1 / T]
    B --&gt; C[For each sample 1..n_samples]
    C --&gt; D[Equilibrate: repeat equilibration_steps sweeps]
    D --&gt; E[Single sweep: pick random site i, compute ΔE, accept/reject flip]
    E --&gt; F[Decorrelate: perform L × steps_between_samples sweeps]
    F --&gt; G[Store current spin configuration]
    G --&gt; C
</code></pre>

<hr />

<h2 id="ml-narrative-transformer-as-boltzmann-distribution-learner">ML Narrative: Transformer as Boltzmann Distribution Learner</h2>

<h3 id="setup">Setup</h3>

<p>We treat each spin configuration ${s_1, \dots, s_L}$ as a <strong>sequence</strong> and train a standard autoregressive Transformer:</p>

\[p_\theta(s_1, \dots, s_L) = \prod_{i=1}^{L} p_\theta(s_i \mid s_{1:i-1})\]

<p><strong>Training objective</strong>: maximum likelihood over equilibrium samples</p>

\[\max_\theta \; \mathbb{E}_{\text{samples}} \Big[\log p_\theta(s_1, \dots, s_L)\Big]\]

<p>If the model has enough capacity and training converges, then:</p>

\[p_\theta(\{s_i\}) \approx p_T(\{s_i\}) = \frac{1}{Z(T)} \exp\Big(-\frac{H[\{s_i\}]}{k_B T}\Big)\]

<p><strong>Key insight</strong>: The Transformer never sees $J$, $h$, or $T$. It only sees raw samples. Yet it learns to <strong>implicitly encode the Boltzmann distribution</strong> and the underlying energy landscape.</p>

<p>The training process, using a standard autoregressive objective, shows a smooth convergence as the model internalizes the underlying physics:</p>

<p><img src="../plots/trainingLog.png" alt="Training loss vs. update" /></p>

<h3 id="architecture-isinggpt">Architecture: IsingGPT</h3>

<pre><code class="language-mermaid">flowchart TD
    A[Input spin configuration s₁ … s_L with values -1 or +1] --&gt; B[Tokenize: map to 0 or 1]
    B --&gt; C[Token embedding layer]
    C --&gt; D[Positional embedding layer]
    D --&gt; E[Stack of Transformer encoder layers with multi-head self-attention and MLP]
    E --&gt; F[Linear readout head]
    F --&gt; G[Logits over 0 or 1 for each site]
</code></pre>

<p><strong>Training loop (conceptual)</strong>:</p>

<pre><code class="language-mermaid">flowchart TD
    A[Initialize IsingGPT parameters theta] --&gt; B[Repeat over epochs]
    B --&gt; C[Sample a batch of equilibrium spin sequences]
    C --&gt; D[Tokenize spins: values -1 or +1 mapped to 0 or 1]
    D --&gt; E[Forward pass through Transformer to obtain logits]
    E --&gt; F[Compute cross-entropy loss vs. true tokens]
    F --&gt; G[Backpropagate gradients for theta loss]
    G --&gt; H[Update parameters theta with optimizer e.g. AdamW]
    H --&gt; B
</code></pre>

<h3 id="what-the-model-learns">What the Model Learns</h3>

<p>After training, we observe:</p>

<ol>
  <li><strong>Accurate Boltzmann distribution</strong>: Sampled configurations from $p_\theta$ match those from Metropolis</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Attention = Correlation function</strong>: The attention weights $\alpha_{ij}$ in layer 1 spontaneously approximate the two-point correlation $C(</td>
          <td>i-j</td>
          <td>)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Phase-transition tracking</strong>: Models trained at different $T$ exhibit systematically different attention patterns</li>
</ol>

<p>Evaluating the trained model across the $(h, T)$ grid reveals robust performance, with degradation near critical points where distributions are more complex:</p>

<p><img src="../plots/report_ht.png" alt="Model report across (h, T)" /></p>

<table>
  <tbody>
    <tr>
      <td><strong>Visualization</strong>: Plot attention matrix $A_{ij} = \alpha_{ij}$ vs. theoretical correlation $C(</td>
      <td>i-j</td>
      <td>)$. They align remarkably well.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="connection-to-statistical-physics">Connection to Statistical Physics</h2>

<h3 id="why-does-this-work">Why does this work?</h3>

<p>The autoregressive factorization</p>

\[p_\theta(s_1, \dots, s_L) = \prod_{i=1}^{L} p_\theta(s_i \mid s_{1:i-1})\]

<p>is <strong>universal</strong>: any distribution over sequences can be written this way. But the Ising model has a special property: it’s a <strong>Markov random field</strong> with nearest-neighbor interactions.</p>

<p>The Transformer’s <strong>attention mechanism</strong> naturally discovers this locality:</p>
<ul>
  <li>At low $T$, spins are strongly correlated → attention focuses on neighbors</li>
  <li>At high $T$, correlations decay fast → attention is more diffuse</li>
</ul>

<p>In other words:</p>
<ul>
  <li><strong>Physics</strong>: $p_T({s_i}) \propto \exp(-H/k_B T)$, where $H$ encodes nearest-neighbor coupling</li>
  <li><strong>ML</strong>: $p_\theta({s_i})$ learned via attention, which discovers the same coupling structure</li>
</ul>

<p>The Transformer <strong>doesn’t know about Hamiltonians</strong>, but by maximizing likelihood on equilibrium samples, it reverse-engineers the energy landscape.</p>

<h3 id="attention-as-a-boltzmann-weight">Attention as a Boltzmann weight</h3>

<p>At each position $i$, the attention score over past spins $s_j$ ($j &lt; i$) can be written:</p>

\[\alpha_{ij} = \frac{\exp(q_i \cdot k_j / \sqrt{d})}{\sum_{j'&lt;i} \exp(q_i \cdot k_{j'} / \sqrt{d})}\]

<p>This is exactly a <strong>local Boltzmann distribution</strong> over indices $j$, where:</p>
<ul>
  <li>“energy” $\approx -q_i \cdot k_j$</li>
  <li>“temperature” $\approx \sqrt{d}$</li>
</ul>

<p>In the Ising model, the relevant information for predicting $s_i$ is $s_{i-1}$ (and to a lesser extent $s_{i-2}, s_{i-3}, \dots$). The attention mechanism automatically <strong>up-weights nearby spins</strong> because they carry higher mutual information.</p>

<p>Empirically, the learned attention weights mirror the theoretical spin-spin correlations, providing direct evidence for this physics-ML connection:</p>

<p><img src="../plots/spin_correlation.png" alt="Spin correlation comparison" /></p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Transformers as implicit Boltzmann machines</strong>: By training on equilibrium samples, the model learns the underlying energy landscape without ever seeing the Hamiltonian.</p>
  </li>
  <li>
    <p><strong>Attention ≈ Correlation</strong>: The learned attention weights naturally mirror the spin-spin correlation function — a purely emergent phenomenon.</p>
  </li>
  <li>
    <p><strong>Statistical physics ↔ ML</strong>: The Ising model is a pedagogical bridge. In more complex systems (language, images, RL), the same principle holds: <strong>maximum likelihood on data ≈ learning the Boltzmann distribution of an unknown energy function</strong>.</p>
  </li>
  <li>
    <p><strong>Phase transitions in neural networks</strong>: The model’s internal representations (attention patterns, layer activations) change qualitatively as you vary the data-generating distribution’s “temperature”. This suggests a deep connection between phase transitions in physics and critical phenomena in deep learning.</p>
  </li>
</ol>

<hr />

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><strong>Physics</strong>: Kardar, <em>Statistical Physics of Particles</em> (Chapter 3: Ising Model)</li>
  <li><strong>ML</strong>: Graves, <em>Generating Sequences With Recurrent Neural Networks</em></li>
  <li><strong>Connection</strong>: Mehta et al., <em>A high-bias, low-variance introduction to Machine Learning for physicists</em> (arXiv:1803.08823)</li>
</ul>]]></content><author><name>ZukSkyWalker</name></author><category term="Transformer" /><category term="Statistical Physics" /><category term="Ising Model" /><category term="Phase Transition" /><summary type="html"><![CDATA[A minimal 2-layer Transformer trained on equilibrium samples from the 1D Ising model spontaneously discovers the Boltzmann distribution, nearest-neighbor spin correlations, and phase-transition behavior — without ever seeing the Hamiltonian.]]></summary></entry></feed>