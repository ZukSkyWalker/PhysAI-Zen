<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Chapter 02: Decision Transformer – Offline RL as Discrete Path Integral - ZukSkyWalker&#39;s Blog</title>
  <meta name="description" content="Offline reinforcement learning recast as sequence modeling: trajectories are &#39;paths&#39; in state-action space, and the Transformer learns a distribution over tr...">
  
  <link rel="stylesheet" href="/PhysAI-Zen/assets/css/style.css">
  <link rel="canonical" href="http://localhost:4000/PhysAI-Zen/2025/12/08/decision-transformer/">
  
  <!-- MathJax for LaTeX support -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  
  <!-- Syntax highlighting - Dark theme -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<body>
  <header class="site-header">
    <div class="wrapper">
      <a class="site-title" href="/PhysAI-Zen/">ZukSkyWalker&#39;s Blog</a>
      <nav class="site-nav">
        <a href="/PhysAI-Zen/">Home</a>
        <a href="https://github.com/ZukSkyWalker" target="_blank">GitHub</a>
      </nav>
    </div>
  </header>

  <main class="page-content">
    <div class="wrapper">
      <article class="post">
  <header class="post-header">
    <h1 class="post-title">Chapter 02: Decision Transformer – Offline RL as Discrete Path Integral</h1>
    <p class="post-meta">
      <time datetime="2025-12-08T00:00:00-08:00">
        December 08, 2025
      </time>
      
        • <span>ZukSkyWalker</span>
      
    </p>
    
      <div class="post-tags">
        
          <span class="tag">Decision Transformer</span>
        
          <span class="tag">Reinforcement Learning</span>
        
          <span class="tag">Path Integral</span>
        
          <span class="tag">Offline RL</span>
        
      </div>
    
  </header>

  <div class="post-content">
    <blockquote>
  <p>Offline reinforcement learning recast as sequence modeling: trajectories are “paths” in state-action space, and the Transformer learns a distribution over trajectories weighted by cumulative reward — exactly analogous to a path integral with action $S = -\sum R_t$.</p>
</blockquote>

<hr />

<h2 id="physics-background">Physics Background</h2>

<h3 id="path-integrals-in-quantum-mechanics">Path Integrals in Quantum Mechanics</h3>

<p>In Feynman’s formulation, the transition amplitude from state $x_0$ at $t=0$ to $x_T$ at time $T$ is:</p>

\[\langle x_T | e^{-iHT/\hbar} | x_0 \rangle = \int \mathcal{D}x(t) \, \exp\Big(\frac{i}{\hbar} S[x(t)]\Big)\]

<p>where the <strong>action</strong> is:</p>

\[S[x(t)] = \int_0^T dt \, \Big[\frac{m}{2}\dot{x}^2 - V(x)\Big]\]

<p>In the <strong>Euclidean (imaginary-time)</strong> version ($t \to -i\tau$), this becomes:</p>

\[Z = \int \mathcal{D}x(\tau) \, \exp\big(-S_E[x(\tau)]\big)\]

<p>with $S_E = \int d\tau \, [\frac{m}{2}\dot{x}^2 + V(x)]$. Now it looks like <strong>statistical mechanics</strong>: paths with lower action have exponentially higher weight.</p>

<h3 id="discrete-path-integrals">Discrete Path Integrals</h3>

<p>For a system evolving in discrete time steps $t = 0, 1, \dots, T$:</p>

\[Z = \sum_{\text{all paths } x_0, x_1, \dots, x_T} \exp\big(-S_{\text{discrete}}[x_0, \dots, x_T]\big)\]

<p>where the discrete action might be:</p>

\[S_{\text{discrete}} = \sum_{t=0}^{T-1} \Big[\frac{(x_{t+1} - x_t)^2}{2\Delta t} + V(x_t) \Delta t\Big]\]

<p><strong>Key idea</strong>: The system “explores all possible paths” but exponentially favors paths with low action.</p>

<h3 id="mapping-physics-to-rl">Mapping Physics to RL</h3>

<p>In the discrete action above, we can identify two components:</p>

<ol>
  <li><strong>Kinetic term</strong> (dynamics): $\frac{(x_{t+1} - x_t)^2}{2\Delta t}$ penalizes large jumps between consecutive states</li>
  <li><strong>Potential term</strong> (cost): $V(x_t) \Delta t$ assigns a cost to visiting state $x_t$</li>
</ol>

<p>In <strong>reinforcement learning</strong>, the analogous structure is:</p>

\[S_{\text{RL}}[\tau] = \sum_{t=0}^{T-1} \Big[\underbrace{-\log p(s_{t+1} | s_t, a_t)}_{\text{dynamics constraint}} + \underbrace{(-r_t)}_{\text{negative reward}}\Big]\]

<p><strong>Correspondence</strong>:</p>
<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Dynamics</strong> $p(s_{t+1}</td>
          <td>s_t, a_t)$ ↔ <strong>Constrained paths</strong>: The MDP transition probabilities act like a “kinetic term” that constrains which state transitions are likely. Deterministic dynamics ($s_{t+1} = f(s_t, a_t)$) correspond to zero kinetic energy—the path is fully constrained.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Reward</strong> $r_t = R(s_t, a_t)$ ↔ <strong>Negative potential</strong>: Reward plays the role of negative potential energy $-V(x_t)$. High reward = low action, making the trajectory more probable.</li>
</ul>

<p>This gives the <strong>trajectory distribution</strong>:</p>

\[p(\tau) = p(s_0) \prod_{t=0}^{T-1} p(s_{t+1} | s_t, a_t) \cdot \pi(a_t | s_t) \cdot \exp\Big(\frac{1}{\alpha} r_t\Big)\]

<table>
  <tbody>
    <tr>
      <td>When we marginalize over the dynamics (which are fixed by the environment), we recover the MaxEnt RL objective where the policy $\pi(a</td>
      <td>s)$ implicitly learns to sample high-reward trajectories.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="rl-as-path-integral-over-trajectories">RL as Path Integral Over Trajectories</h2>

<h3 id="reinforcement-learning-setup">Reinforcement Learning Setup</h3>

<p>An agent interacts with an environment over discrete time steps:</p>
<ul>
  <li><strong>State</strong>: $s_t \in \mathcal{S}$</li>
  <li><strong>Action</strong>: $a_t \in \mathcal{A}$</li>
  <li><strong>Reward</strong>: $r_t = R(s_t, a_t)$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Dynamics</strong>: $s_{t+1} \sim p(s’</td>
          <td>s_t, a_t)$</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>A <strong>trajectory</strong> is a sequence:</p>

\[\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \dots, s_T, a_T, r_T)\]

<p>The <strong>return</strong> (cumulative reward) is:</p>

\[G(\tau) = \sum_{t=0}^{T} \gamma^t r_t\]

<p>where $\gamma \in [0,1]$ is the discount factor.</p>

<h3 id="maximum-entropy-rl-brief-recap-from-chapter-03">Maximum Entropy RL (brief recap from Chapter 03)</h3>

<table>
  <tbody>
    <tr>
      <td>In MaxEnt RL, we seek a policy $\pi(a</td>
      <td>s)$ that maximizes:</td>
    </tr>
  </tbody>
</table>

\[J(\pi) = \mathbb{E}_\tau \Big[\sum_t r_t\Big] + \alpha H(\pi)\]

<p>The optimal policy has the form:</p>

\[\pi^*(a|s) \propto \exp\Big(\frac{1}{\alpha} Q^*(s, a)\Big)\]

<p>This is a <strong>Boltzmann policy</strong> with “temperature” $\alpha$.</p>

<h3 id="trajectory-distribution-as-path-integral">Trajectory Distribution as Path Integral</h3>

<p>We can define a <strong>distribution over trajectories</strong> weighted by return:</p>

\[p(\tau) \propto \exp\Big(\frac{1}{\alpha} G(\tau)\Big)\]

<p>Rewrite $G(\tau) = \sum_t \gamma^t r_t$ as a “negative action”:</p>

\[S[\tau] := -\sum_t \gamma^t r_t\]

<p>Then:</p>

\[p(\tau) = \frac{1}{Z} \exp\big(-S[\tau] / \alpha\big)\]

<p>where $Z = \sum_{\tau} \exp(-S[\tau]/\alpha)$ is the trajectory partition function.</p>

<p><strong>This is exactly the discrete path integral formulation</strong>: trajectories with higher reward (lower “action” $S$) have exponentially higher probability.</p>

<hr />

<h2 id="algorithm-decision-transformer">Algorithm: Decision Transformer</h2>

<p><strong>Key insight</strong> (Chen et al., NeurIPS 2021):<br />
Instead of learning a value function $Q(s,a)$ or policy $\pi(a|s)$, directly model the <strong>conditional distribution</strong>:</p>

\[p(a_t \mid s_0, a_0, r_0, \dots, s_t, \hat{R}_t)\]

<p>where $\hat{R}_t$ is the <strong>desired return-to-go</strong> at time $t$.</p>

<p>At test time:</p>
<ol>
  <li>Specify a high target return $\hat{R}_0$ (e.g., the maximum return seen in the dataset)</li>
  <li>Sample actions autoregressively:
\(a_t \sim p_\theta(a_t \mid s_{0:t}, a_{0:t-1}, r_{0:t-1}, \hat{R}_t)\)</li>
  <li>Update return-to-go: $\hat{R}_{t+1} = \hat{R}_t - r_t$</li>
</ol>

<p><strong>Why this works</strong>: The model learns that “if I’m in state $s$ and want to achieve return $\hat{R}$, I should take action $a$ that historically led to $\hat{R}$ from $s$.”</p>

<h3 id="architecture">Architecture</h3>

<p>Treat the trajectory as a sequence of <strong>interleaved tokens</strong>:</p>

\[(\hat{R}_0, s_0, a_0, \hat{R}_1, s_1, a_1, \dots, \hat{R}_T, s_T, a_T)\]

<p>Use a <strong>causal Transformer</strong> with:</p>
<ul>
  <li><strong>Token embeddings</strong>:
    <ul>
      <li>$\hat{R}_t$: scalar → linear layer → $\mathbb{R}^{d}$</li>
      <li>$s_t$: vector or image → embedding layer → $\mathbb{R}^{d}$</li>
      <li>$a_t$: discrete or continuous → embedding layer → $\mathbb{R}^{d}$</li>
    </ul>
  </li>
  <li><strong>Positional embeddings</strong>: learned embeddings for timestep $t$</li>
  <li><strong>Causal attention</strong>: tokens at time $t$ can only attend to $t’ \leq t$</li>
</ul>

<p>Output: predict $a_t$ given all tokens up to $(s_t, \hat{R}_t)$.</p>

<p><strong>Training objective</strong>: supervised learning on offline trajectories</p>

\[\max_\theta \sum_{t=0}^{T} \log p_\theta(a_t \mid \hat{R}_{0:t}, s_{0:t}, a_{0:t-1})\]

<p>where $\hat{R}<em>t = \sum</em>{t’=t}^{T} \gamma^{t’-t} r_{t’}$ is computed from the recorded trajectory.</p>

<hr />

<h2 id="ml-narrative-trajectory-modeling-vs-dynamic-programming">ML Narrative: Trajectory Modeling vs. Dynamic Programming</h2>

<h3 id="traditional-rl-value-functions">Traditional RL: Value Functions</h3>

<p>Most RL algorithms (Q-learning, Actor-Critic, PPO) learn:</p>
<ul>
  <li><strong>Value function</strong>: $V^\pi(s) = \mathbb{E}<em>\pi[\sum</em>{t} \gamma^t r_t \mid s_0 = s]$</li>
  <li><strong>Q-function</strong>: $Q^\pi(s, a) = \mathbb{E}<em>\pi[\sum</em>{t} \gamma^t r_t \mid s_0 = s, a_0 = a]$</li>
</ul>

<p>These satisfy the <strong>Bellman equation</strong> (dynamic programming):</p>

\[Q^\pi(s, a) = r(s,a) + \gamma \mathbb{E}_{s' \sim p(\cdot|s,a)} [V^\pi(s')]\]

<p>This is a <strong>local, recursive</strong> update: the value at $s$ depends only on the immediate reward and the value of the next state.</p>

<h3 id="decision-transformer-sequence-modeling">Decision Transformer: Sequence Modeling</h3>

<p>Instead, Decision Transformer:</p>
<ol>
  <li>Treats the <strong>entire trajectory</strong> as a sequence</li>
  <li>Uses <strong>global attention</strong> over all timesteps</li>
  <li>Learns the conditional distribution $p(a_t \mid \text{history}, \hat{R}_t)$</li>
</ol>

<p><strong>Advantages</strong>:</p>
<ul>
  <li>No bootstrapping errors (no Bellman backup)</li>
  <li>No off-policy correction needed (just supervised learning)</li>
  <li>Naturally handles long-horizon dependencies (via attention)</li>
  <li>Can be trained on <strong>offline data</strong> without ever interacting with the environment</li>
</ul>

<p><strong>Tradeoffs</strong>:</p>
<ul>
  <li>Requires large, diverse offline datasets</li>
  <li>Doesn’t improve beyond the best trajectory in the dataset</li>
  <li>Less sample-efficient than model-based RL (but more stable)</li>
</ul>

<hr />

<h2 id="connection-to-path-integrals">Connection to Path Integrals</h2>

<h3 id="why-path-integral">Why “Path Integral”?</h3>

<p>In the trajectory distribution</p>

\[p(\tau) \propto \exp\Big(\frac{1}{\alpha} \sum_t \gamma^t r_t\Big),\]

<p>the sum $\sum_t \gamma^t r_t$ plays the role of a <strong>discrete action</strong>. The Decision Transformer implicitly learns this distribution by:</p>
<ol>
  <li>Conditioning on return-to-go $\hat{R}_t$ (≈ “boundary condition” in path integrals)</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Modeling $p(a_t</td>
          <td>s_t, \hat{R}_t, \text{past})$ (≈ “path propagator”)</td>
        </tr>
      </tbody>
    </table>
  </li>
</ol>

<p>At inference, by setting a high $\hat{R}_0$, we <strong>bias the sampling toward high-reward paths</strong>, just as in a path integral, we can bias toward paths with low action by adjusting boundary conditions or sources.</p>

<h3 id="analogy-table">Analogy Table</h3>

<table>
  <thead>
    <tr>
      <th>Path Integral (Physics)</th>
      <th>Decision Transformer (RL)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Path $x(t)$</td>
      <td>Trajectory $\tau = (s_t, a_t, r_t)$</td>
    </tr>
    <tr>
      <td>Action $S[x(t)] = \int L(x, \dot{x}) dt$</td>
      <td>Negative return $S[\tau] = -\sum_t \gamma^t r_t$</td>
    </tr>
    <tr>
      <td>Kinetic term $\frac{m}{2}\dot{x}^2$</td>
      <td>Dynamics constraint $-\log p(s_{t+1} \mid s_t, a_t)$</td>
    </tr>
    <tr>
      <td>Potential $V(x)$</td>
      <td>Negative reward $-r_t$</td>
    </tr>
    <tr>
      <td>Weight $\exp(-S[x]/\hbar)$</td>
      <td>Probability $\exp(\sum_t r_t / \alpha)$</td>
    </tr>
    <tr>
      <td>Partition function $Z = \int \mathcal{D}x \, e^{-S[x]}$</td>
      <td>Trajectory partition function $Z = \sum_\tau e^{G(\tau)/\alpha}$</td>
    </tr>
    <tr>
      <td>Boundary condition $x(0), x(T)$</td>
      <td>Desired return $\hat{R}_0$, initial state $s_0$</td>
    </tr>
    <tr>
      <td>Propagator $K(x_T, x_0)$</td>
      <td>Trajectory distribution $p(\tau \mid s_0, \hat{R}_0)$</td>
    </tr>
  </tbody>
</table>

<p><strong>Key insight</strong>: The Decision Transformer is a <strong>discrete, learned approximation</strong> to the trajectory propagator. By conditioning on return-to-go $\hat{R}_t$, it learns which actions lead to high-reward paths, analogous to how a path integral weights paths by their action.</p>

<table>
  <tbody>
    <tr>
      <td>Note that in the RL setting, the dynamics $p(s_{t+1}</td>
      <td>s_t, a_t)$ are given by the environment and constrain which trajectories are possible (like a “kinetic term” restricting path smoothness), while the policy chooses actions to maximize reward (minimize “potential energy” $-r_t$).</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>RL = Sequence Modeling</strong>: By treating trajectories as sequences and conditioning on return-to-go, we bypass the need for value functions and Bellman equations.</p>
  </li>
  <li>
    <p><strong>Path Integral View</strong>: The trajectory distribution $p(\tau) \propto \exp(G(\tau)/\alpha)$ is mathematically identical to a discrete path integral with action $S = -G(\tau)$.</p>
  </li>
  <li>
    <p><strong>Offline RL Made Simple</strong>: Decision Transformer reduces offline RL to supervised learning — no off-policy corrections, no bootstrapping, just maximum likelihood.</p>
  </li>
  <li>
    <p><strong>Attention = Long-Range Dependencies</strong>: Unlike Markovian methods (Q-learning), the Transformer can discover and exploit correlations across long time horizons.</p>
  </li>
  <li>
    <p><strong>Physics → ML</strong>: The path integral formulation, developed for quantum mechanics, provides a unifying language for understanding both equilibrium statistical mechanics (Chapter 01: Ising) and sequential decision-making (this chapter).</p>
  </li>
</ol>

<hr />

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><strong>Original paper</strong>: Chen et al., <em>Decision Transformer: Reinforcement Learning via Sequence Modeling</em> (NeurIPS 2021)</li>
  <li><strong>Path integrals in RL</strong>: Kappen, <em>Path integrals and symmetry breaking for optimal control theory</em> (2005)</li>
  <li><strong>MaxEnt RL</strong>: Levine, <em>Reinforcement Learning and Control as Probabilistic Inference</em> (arXiv:1805.00909)</li>
  <li><strong>Physics connection</strong>: Feynman &amp; Hibbs, <em>Quantum Mechanics and Path Integrals</em></li>
</ul>


  </div>

  <footer class="post-footer">
    <div class="post-nav">
      
        <a class="prev" href="/PhysAI-Zen/2025/12/01/ising-gpt/">← Chapter 01: IsingGPT – Transformer Learns Phase Transitions</a>
      
      
        <a class="next" href="/PhysAI-Zen/2025/12/10/maxent-rl/">Chapter 03: Maximum-Entropy RL – Langevin Dynamics + Maximum Caliber →</a>
      
    </div>
  </footer>
</article>


    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper">
      <p>Seeking the truth of the universe</p>
      <p>&copy; 2025 Zukai Wang. All rights reserved.</p>
      <p style="font-size: 12px; margin-top: 10px; color: #4a9eff;">May the Force be with you. Always.</p>
    </div>
  </footer>
</body>
</html>

