<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <title>Chapter 01: IsingGPT – Transformer Learns Phase Transitions - ZukSkyWalker&#39;s Blog</title>
  <meta name="description" content="A minimal 2-layer Transformer trained on equilibrium samples from the 1D Ising model spontaneously discovers the Boltzmann distribution, nearest-neighbor spi...">
  
  <link rel="stylesheet" href="/PhysAI-Zen/assets/css/style.css">
  <link rel="canonical" href="http://localhost:4000/PhysAI-Zen/2025/12/01/ising-gpt/">
  
  <!-- MathJax for LaTeX support -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
  </script>
  
  <!-- Syntax highlighting - Dark theme -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/atom-one-dark.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
</head>

<body>
  <header class="site-header">
    <div class="wrapper">
      <a class="site-title" href="/PhysAI-Zen/">ZukSkyWalker&#39;s Blog</a>
      <nav class="site-nav">
        <a href="/PhysAI-Zen/">Home</a>
        <a href="https://github.com/ZukSkyWalker" target="_blank">GitHub</a>
      </nav>
    </div>
  </header>

  <main class="page-content">
    <div class="wrapper">
      <article class="post">
  <header class="post-header">
    <h1 class="post-title">Chapter 01: IsingGPT – Transformer Learns Phase Transitions</h1>
    <p class="post-meta">
      <time datetime="2025-12-01T00:00:00-08:00">
        December 01, 2025
      </time>
      
        • <span>ZukSkyWalker</span>
      
    </p>
    
      <div class="post-tags">
        
          <span class="tag">Transformer</span>
        
          <span class="tag">Statistical Physics</span>
        
          <span class="tag">Ising Model</span>
        
          <span class="tag">Phase Transition</span>
        
      </div>
    
  </header>

  <div class="post-content">
    <blockquote>
  <p>A minimal 2-layer Transformer trained on equilibrium samples from the 1D Ising model spontaneously discovers the Boltzmann distribution, nearest-neighbor spin correlations, and phase-transition behavior — without ever seeing the Hamiltonian. Kudos to Andrej Karpathy’s, this chapter is inspired by his nanoGPT (https://github.com/karpathy/nanoGPT).</p>
</blockquote>

<hr />

<h2 id="physics-background">Physics Background</h2>

<h3 id="the-1d-ising-model">The 1D Ising Model</h3>

<p>The Ising model is the “hydrogen atom” of statistical mechanics: simple enough to solve analytically, rich enough to exhibit nontrivial collective behavior.</p>

<p><strong>Hamiltonian</strong>:</p>

\[H[\{s_i\}] = -J \sum_{i=1}^{L} s_i s_{i+1} - h \sum_{i=1}^{L} s_i\]

<p>where:</p>
<ul>
  <li>$s_i \in {-1, +1}$: spin at site $i$ (periodic boundary: $s_{L+1} = s_1$)</li>
  <li>$J &gt; 0$: ferromagnetic coupling (prefers alignment $\uparrow\uparrow$)</li>
  <li>$h$: external magnetic field</li>
  <li>$L$: chain length</li>
</ul>

<p><strong>Boltzmann distribution</strong> at temperature $T$:</p>

\[p_T(\{s_i\}) = \frac{1}{Z(T)} \exp\Big(-\frac{H[\{s_i\}]}{k_B T}\Big)\]

<p>where $Z(T) = \sum_{{s_i}} \exp(-H/k_B T)$ is the partition function.</p>

<h3 id="phase-transition-in-infinite-1d-none-but-crossover-exists">Phase Transition (in infinite 1D: none; but crossover exists)</h3>

<p>In 1D with finite $L$, there is no true phase transition, but:</p>
<ul>
  <li><strong>High $T$ (paramagnetic)</strong>: spins are disordered, $\langle s_i s_j \rangle \to 0$ rapidly</li>
  <li><strong>Low $T$ (ordered)</strong>: spins align, $\langle s_i s_j \rangle \approx 1$ for all $i, j$</li>
</ul>

<p><strong>Key observable: two-point correlation function</strong></p>

\[C(r) = \langle s_i s_{i+r} \rangle - \langle s_i \rangle^2\]

<p>At $h=0$ (zero field), this decays as</p>

\[C(r) \sim e^{-r/\xi(T)}\]

<p>where $\xi(T) = -1/\log(\tanh(J/k_B T))$ is the <strong>correlation length</strong>. As $T \to 0$, $\xi \to \infty$ (quasi-long-range order).</p>

<p>To illustrate the phase diagram, we generate samples across a grid of temperatures $T$ and fields $h$, verifying that observables like magnetization transition smoothly between ordered (low $T$, small $h$) and disordered (high $T$) regimes:</p>

<p><img src="../plots/data_valid_ht.png" alt="Validation grid over field and temperature" /></p>

<hr />

<h2 id="algorithm-metropolis-hastings-sampling">Algorithm: Metropolis-Hastings Sampling</h2>

<p>We generate equilibrium samples from $p_T({s_i})$ using the <strong>Metropolis algorithm</strong>:</p>

<ol>
  <li>Initialize spins randomly: $s_i \in {-1, +1}$</li>
  <li><strong>Sweep</strong> (repeat $L$ times):
    <ul>
      <li>Pick a random site $i$</li>
      <li>Compute energy change if we flip $s_i \to -s_i$:
\(\Delta E = 2 J s_i (s_{i-1} + s_{i+1}) + 2 h s_i\)</li>
      <li>Accept flip with probability:
\(\min\Big(1, \exp\big(-\Delta E / k_B T\big)\Big)\)</li>
    </ul>
  </li>
  <li>After equilibration (500–1000 sweeps), save configuration</li>
  <li>Wait 10–20 sweeps between samples (decorrelation)</li>
</ol>

<p><strong>Implementation sketch</strong> (<code class="language-plaintext highlighter-rouge">src/ising.py</code>), as a flowchart:</p>

<pre><code class="language-mermaid">flowchart TD
    A[Start: random spins sᵢ taking values -1 or +1] --&gt; B[Set β = 1 / T]
    B --&gt; C[For each sample 1..n_samples]
    C --&gt; D[Equilibrate: repeat equilibration_steps sweeps]
    D --&gt; E[Single sweep: pick random site i, compute ΔE, accept/reject flip]
    E --&gt; F[Decorrelate: perform L × steps_between_samples sweeps]
    F --&gt; G[Store current spin configuration]
    G --&gt; C
</code></pre>

<hr />

<h2 id="ml-narrative-transformer-as-boltzmann-distribution-learner">ML Narrative: Transformer as Boltzmann Distribution Learner</h2>

<h3 id="setup">Setup</h3>

<p>We treat each spin configuration ${s_1, \dots, s_L}$ as a <strong>sequence</strong> and train a standard autoregressive Transformer:</p>

\[p_\theta(s_1, \dots, s_L) = \prod_{i=1}^{L} p_\theta(s_i \mid s_{1:i-1})\]

<p><strong>Training objective</strong>: maximum likelihood over equilibrium samples</p>

\[\max_\theta \; \mathbb{E}_{\text{samples}} \Big[\log p_\theta(s_1, \dots, s_L)\Big]\]

<p>If the model has enough capacity and training converges, then:</p>

\[p_\theta(\{s_i\}) \approx p_T(\{s_i\}) = \frac{1}{Z(T)} \exp\Big(-\frac{H[\{s_i\}]}{k_B T}\Big)\]

<p><strong>Key insight</strong>: The Transformer never sees $J$, $h$, or $T$. It only sees raw samples. Yet it learns to <strong>implicitly encode the Boltzmann distribution</strong> and the underlying energy landscape.</p>

<p>The training process, using a standard autoregressive objective, shows a smooth convergence as the model internalizes the underlying physics:</p>

<p><img src="../plots/trainingLog.png" alt="Training loss vs. update" /></p>

<h3 id="architecture-isinggpt">Architecture: IsingGPT</h3>

<pre><code class="language-mermaid">flowchart TD
    A[Input spin configuration s₁ … s_L with values -1 or +1] --&gt; B[Tokenize: map to 0 or 1]
    B --&gt; C[Token embedding layer]
    C --&gt; D[Positional embedding layer]
    D --&gt; E[Stack of Transformer encoder layers with multi-head self-attention and MLP]
    E --&gt; F[Linear readout head]
    F --&gt; G[Logits over 0 or 1 for each site]
</code></pre>

<p><strong>Training loop (conceptual)</strong>:</p>

<pre><code class="language-mermaid">flowchart TD
    A[Initialize IsingGPT parameters theta] --&gt; B[Repeat over epochs]
    B --&gt; C[Sample a batch of equilibrium spin sequences]
    C --&gt; D[Tokenize spins: values -1 or +1 mapped to 0 or 1]
    D --&gt; E[Forward pass through Transformer to obtain logits]
    E --&gt; F[Compute cross-entropy loss vs. true tokens]
    F --&gt; G[Backpropagate gradients for theta loss]
    G --&gt; H[Update parameters theta with optimizer e.g. AdamW]
    H --&gt; B
</code></pre>

<h3 id="what-the-model-learns">What the Model Learns</h3>

<p>After training, we observe:</p>

<ol>
  <li><strong>Accurate Boltzmann distribution</strong>: Sampled configurations from $p_\theta$ match those from Metropolis</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td><strong>Attention = Correlation function</strong>: The attention weights $\alpha_{ij}$ in layer 1 spontaneously approximate the two-point correlation $C(</td>
          <td>i-j</td>
          <td>)$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li><strong>Phase-transition tracking</strong>: Models trained at different $T$ exhibit systematically different attention patterns</li>
</ol>

<p>Evaluating the trained model across the $(h, T)$ grid reveals robust performance, with degradation near critical points where distributions are more complex:</p>

<p><img src="../plots/report_ht.png" alt="Model report across (h, T)" /></p>

<table>
  <tbody>
    <tr>
      <td><strong>Visualization</strong>: Plot attention matrix $A_{ij} = \alpha_{ij}$ vs. theoretical correlation $C(</td>
      <td>i-j</td>
      <td>)$. They align remarkably well.</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="connection-to-statistical-physics">Connection to Statistical Physics</h2>

<h3 id="why-does-this-work">Why does this work?</h3>

<p>The autoregressive factorization</p>

\[p_\theta(s_1, \dots, s_L) = \prod_{i=1}^{L} p_\theta(s_i \mid s_{1:i-1})\]

<p>is <strong>universal</strong>: any distribution over sequences can be written this way. But the Ising model has a special property: it’s a <strong>Markov random field</strong> with nearest-neighbor interactions.</p>

<p>The Transformer’s <strong>attention mechanism</strong> naturally discovers this locality:</p>
<ul>
  <li>At low $T$, spins are strongly correlated → attention focuses on neighbors</li>
  <li>At high $T$, correlations decay fast → attention is more diffuse</li>
</ul>

<p>In other words:</p>
<ul>
  <li><strong>Physics</strong>: $p_T({s_i}) \propto \exp(-H/k_B T)$, where $H$ encodes nearest-neighbor coupling</li>
  <li><strong>ML</strong>: $p_\theta({s_i})$ learned via attention, which discovers the same coupling structure</li>
</ul>

<p>The Transformer <strong>doesn’t know about Hamiltonians</strong>, but by maximizing likelihood on equilibrium samples, it reverse-engineers the energy landscape.</p>

<h3 id="attention-as-a-boltzmann-weight">Attention as a Boltzmann weight</h3>

<p>At each position $i$, the attention score over past spins $s_j$ ($j &lt; i$) can be written:</p>

\[\alpha_{ij} = \frac{\exp(q_i \cdot k_j / \sqrt{d})}{\sum_{j'&lt;i} \exp(q_i \cdot k_{j'} / \sqrt{d})}\]

<p>This is exactly a <strong>local Boltzmann distribution</strong> over indices $j$, where:</p>
<ul>
  <li>“energy” $\approx -q_i \cdot k_j$</li>
  <li>“temperature” $\approx \sqrt{d}$</li>
</ul>

<p>In the Ising model, the relevant information for predicting $s_i$ is $s_{i-1}$ (and to a lesser extent $s_{i-2}, s_{i-3}, \dots$). The attention mechanism automatically <strong>up-weights nearby spins</strong> because they carry higher mutual information.</p>

<p>Empirically, the learned attention weights mirror the theoretical spin-spin correlations, providing direct evidence for this physics-ML connection:</p>

<p><img src="../plots/spin_correlation.png" alt="Spin correlation comparison" /></p>

<hr />

<h2 id="key-takeaways">Key Takeaways</h2>

<ol>
  <li>
    <p><strong>Transformers as implicit Boltzmann machines</strong>: By training on equilibrium samples, the model learns the underlying energy landscape without ever seeing the Hamiltonian.</p>
  </li>
  <li>
    <p><strong>Attention ≈ Correlation</strong>: The learned attention weights naturally mirror the spin-spin correlation function — a purely emergent phenomenon.</p>
  </li>
  <li>
    <p><strong>Statistical physics ↔ ML</strong>: The Ising model is a pedagogical bridge. In more complex systems (language, images, RL), the same principle holds: <strong>maximum likelihood on data ≈ learning the Boltzmann distribution of an unknown energy function</strong>.</p>
  </li>
  <li>
    <p><strong>Phase transitions in neural networks</strong>: The model’s internal representations (attention patterns, layer activations) change qualitatively as you vary the data-generating distribution’s “temperature”. This suggests a deep connection between phase transitions in physics and critical phenomena in deep learning.</p>
  </li>
</ol>

<hr />

<h2 id="further-reading">Further Reading</h2>

<ul>
  <li><strong>Physics</strong>: Kardar, <em>Statistical Physics of Particles</em> (Chapter 3: Ising Model)</li>
  <li><strong>ML</strong>: Graves, <em>Generating Sequences With Recurrent Neural Networks</em></li>
  <li><strong>Connection</strong>: Mehta et al., <em>A high-bias, low-variance introduction to Machine Learning for physicists</em> (arXiv:1803.08823)</li>
</ul>


  </div>

  <footer class="post-footer">
    <div class="post-nav">
      
      
        <a class="next" href="/PhysAI-Zen/2025/12/08/decision-transformer/">Chapter 02: Decision Transformer – Offline RL as Discrete Path Integral →</a>
      
    </div>
  </footer>
</article>


    </div>
  </main>

  <footer class="site-footer">
    <div class="wrapper">
      <p>Seeking the truth of the universe</p>
      <p>&copy; 2025 Zukai Wang. All rights reserved.</p>
      <p style="font-size: 12px; margin-top: 10px; color: #4a9eff;">May the Force be with you. Always.</p>
    </div>
  </footer>
</body>
</html>

